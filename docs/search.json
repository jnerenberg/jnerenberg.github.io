[
  {
    "objectID": "posts/women in data science conference/Women-in-Data-Science.html",
    "href": "posts/women in data science conference/Women-in-Data-Science.html",
    "title": "Women in Data Science Conference",
    "section": "",
    "text": "CSCI 0451: Women in Data Science Conference\n\nAbstract\nThis blog post is a summary of and reflection of the Women in Data Science Conference that I attended on March 4, 2024. The conference was held at Middlebury College and was organized by Breanna Guo, a senior. The conference featured several speakers, including keynote speaker Sarah Brown, a Computer Science professor at the University of Rhode Island. There were many familiar faces at the conference, including several of my classmates and professors. It was a great opportunity to learn more about the field of data science and to connect with my professors and peers.\n\n\nWhy Spotlight Women in Data Science?\nUnderrepresentation of women in computing, math, and engineering contributes to a significant loss of talent and innovation in these fields. The lack of diversity in these fields is a problem because it limits the perspectives and ideas to those centered around the male demographic. This is a problem for everyone, as it means that the products and services that are created are not as inclusive or as innovative as they could be.\nThe representation and status of women in computing today differs from the 1950s and 1960s in that it has gone from a role that was predominantly filled by women to one that is predominantly filled by men. Initially, computing positions were seen as low-status, low-paying jobs that were considered to be “women’s work.” In a way, it was seen as similar to secretary work. However, as the field of computing grew and became more visible, men began to be favored for these positions as they were often seen as more competent and capable. The advent of the personal computer also played a role in this shift, as it was marketed towards gamers and the “anti-social nerd” male stereotype. This led to men being more likely to pursue careers in computing, having been encouraged by this stereotype and exposed to computers at a young age.\nMany of the barriers and unequal challenges attributed to the underrepresentation of women in computing can be eroded by events that spotlight the achievement of women in STEM. One of the main barriers is the lack of female role models and mentors in these fields. The achievements of the modern women in computer can serve to inspire and motivate young women to pursue careers in computing. Additionally, the stereotypes barring women from these fields can be broken down by showcasing their accomplishments. It is a compounding success, as the more women are spotlighted in these fields, the more likely it is that women will be encouraged to pursue careers in computing, and the more likely it is that the stereotypes will be broken down.\n\n\nThe Conference\nThe first lightning talk of the conference was Professor Amy Yuen, a Political Science professor at Middlebury College. Professor Yuen spoke about the importance of data science in the field of political science. She explained her work in using data science to analyze the United Nation’s Security Council membership changes over time. With only 15 members on the UN Security Council and 5 of which being permanent members, sponsorship plays a significant role in the election of non-permanent members. Professor Yuen argued that while this method may seem un-representative, the seats that are campaigned for general have somewhat equitable representation. This application of data science in the political sector is integral to understanding the motives of the UN and the power dynamics shaping our world.\nThe next talk was by the keynote speaker, Professor Sarah Brown, a Computer Science professor at the University of Rhode Island. Professor Brown spoke about her experiences in the field of data science as a multi-disciplinary field. She argued that data science is not just about the data, but about the people who are using the data. She emphasized the importance of collaboration between data scientists and domain experts, as well as the importance of communication skills in data science. Professor Brown also discussed diversity in the field, as it leads to more innovative and inclusive solutions. I learned that data science is not just about the data, but about the people who are using the data. I also learned that collaboration and communication are key skills in data science, and that diversity is important for creating innovative and inclusive solutions.\nProfessor Brown’s presentation was organized around three keys: contextualize data, disciplines are communities, and meet people where they are. She explained how data is a primary source of information and needs to be examined with a critical eye, much like how one would in a social studies environment. She also expressed the importance of seeking the expertise of individuals rooted within the field of study, as they can provide valuable insights that data alone cannot. Lastly, she emphasized the importance of meeting people where they are. She discussed her experiences on the board of the National Society for Black Engineers and how she realized the barriers between policy change at the higher levels and policy enaction at the lower levels. These keys, while each seeming to be a small piece of the puzzle, form a strong basis for fair and effective data science practices.\nProfessor Brown’s talk was followed by a lightning talk by Professor Jessica L’Roe, a professor of geography at Middlebury college. Her talk was focused on her experiences in using data science to analyze the usage of farmland around the world. She focused on the impact of shifts in farmland usage on local communities. She found that shifts in farmland ownership from indigenous communities to large corporations cause many families to encourage their youth to leave the farming industry. Another interesting find by Professor L’Roe concerned the reporting of farmland area owned by entities. She found that there was a much larger concentration of farms approaching the upper limits of regulations than the lower limits. While this could be a result of purchasing land in specific increments, it most likely suggests that there is a significant amount of underreporting of farmland ownership by those who own more land. Professor L’Roe’s work was a great example of how data science can be used to analyze and understand complex socio-economic and environmental issues.\nThe final talk of the conference was by Professor Laura Biester, a professor of computer science at Middlebury College. Professor Biester spoke primarily about her work in the natural language processing field, and shared some of her research on emotion detection in text posts. Amazingly, she was able to compile a dataset of almost every reddit post. Using this data, she was able to work on detection and prediction of depression in text posts. In order to ethically do this, Professor Biester relied on self reported diagnoses of depression within the dataset. Interestingly, despite the fact that these self reported diagnoses could be fabricated, those reports are few and far between, and had no significant impact on the study. Professor Biester’s work concluded by being able to somewhat accurately predict depression diagnoses from textual features in written works. In the future, this sort of work can be used to help identify individuals who may be at risk for developing depression and provide them with early intervention.\n\n\nReflection\nI was inspired by the WiDS conference. It reminded me of my reasons for choosing Computer Science as a major: it is applicable to any field. Often, computer science is seen as a field that is separate from other fields, but the WiDS conference demonstrated its unique ability to bridge the gaps between disciplines. I have always considered myself to be a multi-disciplinary person, and value adding skillsets to my toolbox. Data and computer science are the tools that facilitate this goal. In particular, it was enlightening to see the work of Professor Biester outside of the classroom. As a student of hers, I was struck by the realization that her work could one day directly benefit someone like me. It was a reminder that the work we do in the classroom is not just for the sake of learning, but for the sake of making a difference in the world. I was also stunned by how close we as students of computer science are to being able to do some of this same work. The tools and techniques that we are learning in our classes are the same ones that are being used by professionals in the field; a great reminder that we are not just students, but future professionals."
  },
  {
    "objectID": "posts/optimal decision making/'Optimal' Decision-Making.html",
    "href": "posts/optimal decision making/'Optimal' Decision-Making.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "CSCI 0451: ‘Optimal’ Decision Making\n\nIntroduction\nThis post explores the concept of ‘optimal’ decision making in the context of computer science. We will discuss the concept of ‘optimal’ and how it can affect the decision-making process. We will also explore some of the challenges and limitations of ‘optimal’ decision making in computer science.\nI use a logistic regression model to predict whether a loan will be paid off or not. I then calculate a threshold for the model to maximize profit. I then use this threshold to make decisions on whether to lend money to a borrower or not.\nTo spoil the ending, with money-minded decision making, we were able to profit almost $3000 per loan. However, this doesn’t take into account any of ethical considerations that come with not lending money. This is a very important consideration to make when making decisions in the real world, as many decisions are not as simple as maximizing profit.\nLet’s begin by grabbing the data:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\nExploring the Data\nThe first step is to look at the data: how it looks, what it contains, and initial observations. Lets take a look.\n\n# data exploration\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\ndf_train.describe()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\n\n\ncount\n26064.000000\n2.606400e+04\n25351.000000\n26064.000000\n23566.000000\n26064.000000\n26064.000000\n26064.000000\n\n\nmean\n27.734385\n6.597481e+04\n4.783559\n9569.468424\n11.009372\n0.217580\n0.169970\n5.794467\n\n\nstd\n6.362612\n6.330551e+04\n4.166275\n6297.660303\n3.246762\n0.412608\n0.106491\n4.055432\n\n\nmin\n20.000000\n4.000000e+03\n0.000000\n500.000000\n5.420000\n0.000000\n0.000000\n2.000000\n\n\n25%\n23.000000\n3.887250e+04\n2.000000\n5000.000000\n7.900000\n0.000000\n0.090000\n3.000000\n\n\n50%\n26.000000\n5.500000e+04\n4.000000\n8000.000000\n10.990000\n0.000000\n0.150000\n4.000000\n\n\n75%\n30.000000\n7.900000e+04\n7.000000\n12000.000000\n13.470000\n0.000000\n0.230000\n8.000000\n\n\nmax\n144.000000\n6.000000e+06\n123.000000\n35000.000000\n23.220000\n1.000000\n0.830000\n30.000000\n\n\n\n\n\n\n\nFirst off, I’m very interested in the maximum age being 144. For the sake of my waning amusement, lets take a closer look at them.\n\nprint(\"Entry with maximum person_age:\\n\", df_train.loc[df_train['person_age'].idxmax()])\n\nEntry with maximum person_age:\n person_age                        144\nperson_income                  250000\nperson_home_ownership            RENT\nperson_emp_length                 4.0\nloan_intent                   VENTURE\nloan_grade                          C\nloan_amnt                        4800\nloan_int_rate                   13.57\nloan_status                         0\nloan_percent_income              0.02\ncb_person_default_on_file           N\ncb_person_cred_hist_length          3\nName: 1561, dtype: object\n\n\nFascinating! Moving on, let’s make some visualizations to better understand the data.\n\n# let's make some visualizations to better understand the data\nimport matplotlib.pyplot as plt\n\n# histogram of person_age\nplt.hist(df_train['person_age'], bins = 20)\nplt.xlabel('Person Age')\nplt.ylabel('Frequency')\nplt.title('Histogram of Person Age')\nplt.show()\n\n# histogram of person_income\nplt.hist(df_train['person_income'], bins = 20)\nplt.xlabel('Person Income')\nplt.ylabel('Frequency')\nplt.title('Histogram of Person Income')\nplt.show()\n\n# histogram of loan_int_rate\nplt.hist(df_train['loan_int_rate'], bins = 20)\nplt.xlabel('Loan Interest Rate')\nplt.ylabel('Frequency')\nplt.title('Histogram of Loan Interest Rate')\nplt.show()\n\n# histogram of loan_percent_income\nplt.hist(df_train['loan_percent_income'], bins = 20)\nplt.xlabel('Loan Percent Income')\nplt.ylabel('Frequency')\nplt.title('Histogram of Loan Percent Income')\nplt.show()\n\n# histogram of cb_person_cred_hist_length\nplt.hist(df_train['cb_person_cred_hist_length'], bins = 20)\nplt.xlabel('Credit History Length')\nplt.ylabel('Frequency')\nplt.title('Histogram of Credit History Length')\nplt.show()\n\n# bar plot of loan_status\ndf_train['loan_status'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Status')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Status')\nplt.show()\n\n# bar plot of loan_intent\ndf_train['loan_intent'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Intent')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Intent')\nplt.show()\n\n# bar plot of person_home_ownership\ndf_train['person_home_ownership'].value_counts().plot(kind='bar')\nplt.xlabel('Home Ownership')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Home Ownership')\nplt.show()\n\n# bar plot of loan_grade\ndf_train['loan_grade'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Grade')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Grade')\nplt.show()\n\n# bar plot of cb_person_default_on_file\ndf_train['cb_person_default_on_file'].value_counts().plot(kind='bar')\nplt.xlabel('Default on File')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Default on File')\nplt.show()\n\n# bar plot of cb_person_cred_hist_length\ndf_train['cb_person_cred_hist_length'].value_counts().plot(kind='bar')\nplt.xlabel('Credit History Length')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Credit History Length')\nplt.show()\n\n# bar plot of person_emp_length\ndf_train['person_emp_length'].value_counts().plot(kind='bar')\nplt.xlabel('Employment Length')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Employment Length')\nplt.show()\n\n# bar plot of loan_amnt\ndf_train['loan_amnt'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Amount')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Amount')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems that a lot of the data is concentrated around the mean. This is a good sign, as it means that the data is not too spread out. This will make it easier to analyze and make decisions based on general trends.\nSince we are interested in determining the ‘optimal’ decision, we will need to define what ‘optimal’ means in this context. In the most simplified terms of loans, we want to minimize the risk of default while maximizing the return on investment. This is a classic trade-off problem in computer science, and we will need to balance these two objectives to make the ‘optimal’ decision. However, it is important to note that this is a gross generalization and not necessarily indicative of individual cases.\n\n\nBuilding a Model\nTo build a model, we will need to define the features and labels of the data. The features are the attributes of the data that we will use to make predictions, excluding loan_garde, while the labels are the outcomes that we are trying to predict. In this case, the features are the attributes of the loans, such as the loan amount, interest rate, and term length, while the labels are the outcomes of the loans, such as whether the loan was repaid or defaulted.\nWe will use a Logistic Regression model to do this. Lets try to train the model on all of the attributes excluding loan_grade and loan_status and see how well it performs.\n\n# We will use a Logistic Regression model to do this. Lets try to train the model on all of the attributes excluding loan_grade and loan_status and see how well it performs.\n\nfrom sklearn.linear_model import LogisticRegression\n\nX_train = df_train\n\n# drop loan_grade\nX_train = X_train.drop(['loan_grade'], axis = 1)\n\n# drop loan_status\nX_train = X_train.drop(['loan_status'], axis = 1)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\n1200\n9.91\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\n11750\n13.47\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\n10000\n7.51\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\n1325\n12.87\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\n15000\n9.63\n0.28\nN\n10\n\n\n\n\n\n\n\n\n# one-hot encode the categorical variables\nX_train = pd.get_dummies(X_train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n0\n25\n43200\nNaN\n1200\n9.91\n0.03\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n# encode true/false columns as 1/0\nX_train['person_home_ownership_MORTGAGE'] = X_train['person_home_ownership_MORTGAGE'].astype(int)\nX_train['person_home_ownership_OTHER'] = X_train['person_home_ownership_OTHER'].astype(int)\nX_train['person_home_ownership_OWN'] = X_train['person_home_ownership_OWN'].astype(int)\nX_train['person_home_ownership_RENT'] = X_train['person_home_ownership_RENT'].astype(int)\nX_train['loan_intent_DEBTCONSOLIDATION'] = X_train['loan_intent_DEBTCONSOLIDATION'].astype(int)\nX_train['loan_intent_EDUCATION'] = X_train['loan_intent_EDUCATION'].astype(int)\nX_train['loan_intent_HOMEIMPROVEMENT'] = X_train['loan_intent_HOMEIMPROVEMENT'].astype(int)\nX_train['loan_intent_MEDICAL'] = X_train['loan_intent_MEDICAL'].astype(int)\nX_train['loan_intent_PERSONAL'] = X_train['loan_intent_PERSONAL'].astype(int)\nX_train['loan_intent_VENTURE'] = X_train['loan_intent_VENTURE'].astype(int)\nX_train['cb_person_default_on_file_N'] = X_train['cb_person_default_on_file_N'].astype(int)\nX_train['cb_person_default_on_file_Y'] = X_train['cb_person_default_on_file_Y'].astype(int)\n\n# drop NAs\nX_train = X_train.dropna()\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n# train the model\n# Choose your features, estimate new ones if needed, and fit a score-based machine learning model to the data. My suggestion is LogisticRegression. Once you have fit a logistic regression model, the weight vector is stored as the attribute model.coef_.\nY_train = df_train['loan_status']\nY_train = Y_train[X_train.index]\n\nmodel = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, Y_train)\n\n# get the weights\nweights = model.coef_\nweights = weights[0]\nweights\n\narray([-6.36230941e-08, -4.05736686e-05, -2.49216289e-08,  1.06558109e-04,\n        9.49857325e-08,  2.54996456e-09, -1.22515188e-08, -6.57168949e-09,\n        5.82453376e-11, -3.93934560e-09,  8.07258494e-09,  2.68000657e-09,\n       -3.01882534e-09,  1.83914839e-09,  1.33270518e-09, -1.41426169e-09,\n       -3.79897794e-09, -9.65224646e-09,  7.27204165e-09])\n\n\nThe weights tell us how much each feature contributes to the prediction. The larger the weight, the more important the feature is in determining the outcome. We can use these weights to make predictions on new data and determine the ‘optimal’ decision.\n\n# test the model with cross-validation\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, Y_train, cv = 5)\n\narray([0.80379747, 0.81209079, 0.82558393, 0.80571928, 0.80921196])\n\n\nThe model seems to be performing well, with cross validation scores averaging to about 0.81. This is a good sign, but we need to now find a threshold to determine whether a loan should be approved or not. This is where the concept of ‘optimal’ decision making comes into play. There are two assumptions we should make to determine whether a loan should be approved or not:\nThe first is that the loan is repaid in full, and is represented by \\(loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt\\). This assumes that the loan is repaid in full after 10 years, and that the lender is able to recover 25% of the loan amount as interest after paying employees and other expenses.\nThe second is that the loan is defaulted on, and is represented by \\(loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt\\). This assumes that the loan is defaulted on after 3 years, and that the lender is able to recover 30% of the loan amount.\nWe can use these two assumptions to determine the threshold for the ‘optimal’ decision. If the predicted outcome is greater than the threshold, then the loan should be approved, otherwise it should be rejected.\n\nimport numpy as np\n\n# create functions to calculate profit and loss for an entry in the dataset\n\ndef profitORloss(row):\n    # access loan status from Y_train\n    loan_status = Y_train.loc[row.name]\n    # if loan_status is 0, calculate profit, else calculate loss\n    if loan_status == 0:\n        return row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)**10 - row['loan_amnt']\n    else:\n        return -(row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)*3 - 1.7*row['loan_amnt'])\n\n\n\n# calculate profit or loss for each entry in the dataset and add it to the new column 'profit_or_loss'\nX_train['profit_or_loss'] = X_train.apply(profitORloss, axis = 1)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n4613.567568\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n2044.334031\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n-1850.395625\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n4028.690420\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n-7765.037500\n\n\n\n\n\n\n\n\n# graphic of profit or loss for the entire dataset\nplt.hist(X_train['profit_or_loss'], bins = 20)\nplt.xlabel('Profit or Loss')\nplt.ylabel('Frequency')\nplt.title('Histogram of Profit or Loss')\nplt.show()\n\n# calculate the total profit or loss for the entire dataset\ntotal_profit_or_loss = X_train['profit_or_loss'].sum()\nprint(\"Total Profit or Loss: \", total_profit_or_loss)\n\n\n\n\n\n\n\n\n-24572766.55886856\n\n\nNow we can see the profit (or lack thereof) of each loan. Based on these assumptions, we can determine the threshold \\(t\\) for the ‘optimal’ decision, where if the profit is greater than \\(t\\), then the loan should be approved, otherwise it should be rejected. Now, lets graph the profit of each loan against different thresholds to determine the ‘optimal’ decision.\n\n# Now we can see the profit (or lack thereof) of each loan. Based on these assumptions, we can determine the threshold $t$ for the 'optimal' decision, where if the profit is greater than $t$, then the loan should be approved, otherwise it should be rejected. Now, lets graph the profit of each loan against different thresholds to determine the 'optimal' decision.\n\n# find maximum profit\nmax_profit = 0\n\nfor t in np.linspace(-5000, 5000, 100):\n    profit = X_train['profit_or_loss']\n    profit = profit[profit &gt; t]\n    profit = profit.sum()\n    plt.scatter(t, profit, color = 'blue')\n    if profit &gt; max_profit:\n        max_profit = profit\n        max_t = t\nplt.xlabel('Threshold')\nplt.ylabel('Profit')\nplt.title('Profit vs. Threshold')\n# plot the threshold at maximum profit\nplt.axvline(x = max_t, color = 'red')\nplt.show()\n\nprint(\"Threshold at maximum profit: \", max_t)\nprint(\"Maximum profit: \", max_profit)\n\n\n\n\n\n\n\n\nThreshold at maximum profit:  -1262.6262626262628\nMaximum profit:  50918882.99425644\n\n\n\n# add a new column 'decision' to the dataset\nX_train['decision'] = X_train['profit_or_loss'] &gt; max_t\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss\ndecision\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n1\n4613.567568\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n1\n0\n2044.334031\nTrue\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n1\n0\n-1850.395625\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n4028.690420\nTrue\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n-7765.037500\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\nWith this threshold, we can now determine the ‘optimal’ decision for each loan. If the profit is greater than the threshold, then the loan should be approved, otherwise it should be rejected. Using that decision, we can determine the overall profit of the loans and evaluate the performance of the model.\n\n# With this threshold, we can now determine the 'optimal' decision for each loan. If the profit is greater than the threshold, then the loan should be approved, otherwise it should be rejected. Using that decision, we can determine the overall profit of the loans and evaluate the performance of the model.\n\n# calculate the overall profit of the loans\noverall_profit = X_train['profit_or_loss'][X_train['decision']].sum()\nprint(\"Overall Profit: \", overall_profit)\n\n# calculate average profit per accepted loan\naverage_profit = overall_profit/X_train['decision'].sum()\nprint(\"Average Profit per Accepted Loan: \", average_profit)\n\nOverall Profit:  50918882.99425644\nAverage Profit per Accepted Loan:  2831.8159720959034\n\n\n\n\nEvaluating the Money Making Model\nNow, lets evaluate the model on the test set to see how well it performs. We will use the threshold that we determined earlier to make predictions on the test set and evaluate the performance of the model.\n\n# test model on test data\n# load test data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n# drop NAs\ndf_test = df_test.dropna()\nX_test, Y_test = df_test.drop(['loan_status'], axis = 1), df_test['loan_status']\n\n# drop loan_grade\nX_test = X_test.drop(['loan_grade'], axis = 1)\n\n# one-hot encode the categorical variables\nX_test = pd.get_dummies(X_test)\n\n# encode true/false columns as 1/0\nX_test['person_home_ownership_MORTGAGE'] = X_test['person_home_ownership_MORTGAGE'].astype(int)\nX_test['person_home_ownership_OTHER'] = X_test['person_home_ownership_OTHER'].astype(int)\nX_test['person_home_ownership_OWN'] = X_test['person_home_ownership_OWN'].astype(int)\nX_test['person_home_ownership_RENT'] = X_test['person_home_ownership_RENT'].astype(int)\nX_test['loan_intent_DEBTCONSOLIDATION'] = X_test['loan_intent_DEBTCONSOLIDATION'].astype(int)\nX_test['loan_intent_EDUCATION'] = X_test['loan_intent_EDUCATION'].astype(int)\nX_test['loan_intent_HOMEIMPROVEMENT'] = X_test['loan_intent_HOMEIMPROVEMENT'].astype(int)\nX_test['loan_intent_MEDICAL'] = X_test['loan_intent_MEDICAL'].astype(int)\nX_test['loan_intent_PERSONAL'] = X_test['loan_intent_PERSONAL'].astype(int)\nX_test['loan_intent_VENTURE'] = X_test['loan_intent_VENTURE'].astype(int)\nX_test['cb_person_default_on_file_N'] = X_test['cb_person_default_on_file_N'].astype(int)\nX_test['cb_person_default_on_file_Y'] = X_test['cb_person_default_on_file_Y'].astype(int)\n\n\n\n# predict the loan status\nY_pred = model.predict(X_test)\n\n# calculate accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of model: \", accuracy_score(Y_test, Y_pred))\n\n# ...\n\n# calculate profit or loss for each entry in the dataset and add it to the new column 'profit_or_loss'\ndef profitORloss2(row, loan_status):\n    if loan_status == 0:\n        return row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)**10 - row['loan_amnt']\n    else:\n        return -(row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)*3 - 1.7*row['loan_amnt'])\n\n# Reset the index of X_test\nX_test.reset_index(drop=True, inplace=True)\n# Reset the index of Y_test\nY_test.reset_index(drop=True, inplace=True)\n\nX_test['profit_or_loss_predicted'] = X_test.apply(lambda row: profitORloss2(row, Y_pred[row.name]), axis=1)\n\n# add a new column 'decision' to the dataset\nX_test['decision_from_predicted'] = X_test['profit_or_loss_predicted'] &gt; max_t\n\nX_test.head()\n\nAccuracy of model:  0.800383877159309\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss_actual\nprofit_or_loss_predicted\ndecision_from_predicted\ndecision_from_actual\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n-1416.850000\n465.367227\nTrue\nFalse\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\n1\n0\n0\n...\n0\n0\n0\n0\n1\n0\n4847.780062\n4847.780062\nTrue\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n1091.841800\n1091.841800\nTrue\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\n1\n0\n0\n...\n0\n1\n0\n0\n1\n0\n-16869.900000\n4972.214553\nTrue\nFalse\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\n0\n0\n0\n...\n0\n1\n0\n0\n1\n0\n-9933.700000\n3331.859215\nTrue\nFalse\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# calculate profit on test data\nprofit = X_test['profit_or_loss_predicted']\nprofit = profit[profit &gt; max_t]\nprofit = profit.sum()\nprofit\n\n# calculate profit on test data if all loans are approved\nprofit_all_approved = X_test['profit_or_loss_predicted'].sum()\nprint(\"Profit if all loans are approved: \", profit_all_approved)\n\n# calculate profit on test data if all loans are rejected\nprofit_all_rejected = 0\nprint(\"Profit if all loans are rejected: \", profit_all_rejected)\n\n# calculate profit on test data if we use the model to make decisions\nprofit_model = profit\nprint(\"Profit if we use the model to make decisions: \", profit_model)\n\n# improvement in profit\nimprovement = profit_model - profit_all_approved\nprint(\"Improvement in profit: \", improvement)\n\n# average profit per loan\naverage_profit = profit/len(X_test)\nprint(\"Average profit per loan: \", average_profit)\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(Y_test, X_test['decision_from_predicted'])\n\nProfit if all loans are approved:  9487460.66068708\nProfit if all loans are rejected:  0\nProfit if we use the model to make decisions:  16509810.33443708\nImprovement in profit:  7022349.67375\nAverage profit per loan:  2880.7904963247393\n\n\narray([[  77, 4377],\n       [ 210, 1067]], dtype=int64)\n\n\nThats pretty sweet! We have an accuracy of 80% on the test set, which is pretty high. We also increased the projected profits from 9487460 to 16509810, which is a 7 million dollar increase. This is a significant improvement, and shows the power of ‘optimal’ decision making in money driven computer science. It was also interesting to look at the confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives. This can help us understand where the model is making mistakes and how we can improve it, however, I’m feeling pretty good about 7 million dollars.\nAdditionally, the average profit per loan actually increased in the test set, though only marginally. It went from 2832 to 2881, so 49 dollars. I guess the money business is booming!\n\n\nEvaluating the Model Morally\nMoney is great, but now it is time to evaluate how the model might discriminate against certain groups. To do this, I will graph the distribution of rejected and accepted loans for different metrics. I will be attempting to answer 3 questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\n# Is it more difficult for people in certain age groups to access credit under your proposed system?\n# calculate the percentage of approved loans for each age\napprovedByAge = []\nfor i in range(18, 101):\n    approvedByAge.append(X_test['decision_from_predicted'][X_test['person_age'] == i].sum()/len(X_test['decision_from_predicted'][X_test['person_age'] == i]))\nactualByAge = []\n# add Y_test to X_test\nX_test['loan_status'] = abs(Y_test -1) \nfor i in range(18, 101):\n    actualByAge.append(X_test['loan_status'][X_test['person_age'] == i].sum()/len(X_test['loan_status'][X_test['person_age'] == i]))\nplt.plot(range(18, 101), approvedByAge, color = 'red')\nplt.plot(range(18, 101), actualByAge, color = 'blue')\nplt.xlabel('Age')\nplt.ylabel('Predicted vs actual repayment rate')\nplt.show()\n\n# graph of difference between predicted and actual repayment rate by age\nplt.plot(range(18, 101), np.array(approvedByAge) - np.array(actualByAge))\nplt.xlabel('Age')\nplt.ylabel('Difference between predicted and actual repayment rate')\nplt.show()\n\nC:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3012\\4166889995.py:5: RuntimeWarning: invalid value encountered in scalar divide\n  approvedByAge.append(X_test['decision_from_predicted'][X_test['person_age'] == i].sum()/len(X_test['decision_from_predicted'][X_test['person_age'] == i]))\nC:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3012\\4166889995.py:10: RuntimeWarning: invalid value encountered in scalar divide\n  actualByAge.append(X_test['loan_status'][X_test['person_age'] == i].sum()/len(X_test['loan_status'][X_test['person_age'] == i]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like younger people are expected to default more often, which is why they are rejected more often. There is also an interesting trend where older aged individuals are more likely to have loans accepted that will default. I wonder if this is because they are likely to die prior to full repayment of the loan. The implications of this are interesting, as it could be seen as a form of discrimination against both younger and older individuals, even if it is based on data. Overall though, the model does a decent job of not completely rejecting any age group, which is a good sign.\n\n# Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\n# calculate the percentage of approved loans for each loan intent\napprovedByIntent = []\nX_test['loan_intent'] = df_test['loan_intent']\nfor intent in X_test['loan_intent'].unique():\n    approvedByIntent.append(X_test['decision_from_predicted'][X_test['loan_intent'] == intent].sum()/len(X_test['decision_from_predicted'][X_test['loan_intent'] == intent]))\nactualByIntent = []\nfor intent in X_test['loan_intent'].unique():\n    actualByIntent.append(X_test['loan_status'][X_test['loan_intent'] == intent].sum()/len(X_test['loan_status'][X_test['loan_intent'] == intent]))\n\n# table of predicted vs actual repayment rate by loan intent\n\ndf = pd.DataFrame({'Loan Intent': X_test['loan_intent'].unique(), 'Predicted Repayment Rate': approvedByIntent, 'Actual Repayment Rate': actualByIntent})\nprint(df)\n\n# graph of predicted vs actual repayment rate by loan intent\n\nplt.bar(X_test['loan_intent'].unique(), approvedByIntent, color = 'red')\nplt.bar(X_test['loan_intent'].unique(), actualByIntent, color = 'blue')\nplt.xlabel('Loan Intent')\nplt.xticks(rotation = 45)\nplt.ylabel('Predicted vs actual repayment rate')\nplt.legend(['Approved', 'Actual'])\nplt.show()\n\n         Loan Intent  Predicted Repayment Rate  Actual Repayment Rate\n0            VENTURE                  0.954988               0.765207\n1          EDUCATION                  0.949564               0.804074\n2            MEDICAL                  0.947368               0.767368\n3    HOMEIMPROVEMENT                  0.947080               0.753650\n4           PERSONAL                  0.944764               0.757192\n5  DEBTCONSOLIDATION                  0.955112               0.812968\n\n\n\n\n\n\n\n\n\nIt seems like the loans for all of the different purposes are accepted at similar rates, which is a good sign. However, the actual default rates for each group are a little different. For medical expenses, model predicts that more of the loans will be repayed than they actually are. This is a good sign, but medical loans are still the third most likely to be denied but only the 4th most likely to default. However, the difference in approval for each group is not too large. Despite this, medical debt is undoubtedly a horrible burden upon those who are already suffering, and it is important to consider the additional human cost of denying these loans.\n\n# How does a person’s income level impact the ease with which they can access credit under your decision system?\n\n# divide income into 10 bins\nX_test['income_bin'] = pd.qcut(X_test['person_income'], 10, labels = False)\n\n# calculate the percentage of approved loans for each income bin\napprovedByIncome = []\nfor i in range(10):\n    approvedByIncome.append(X_test['decision_from_predicted'][X_test['income_bin'] == i].sum()/len(X_test['decision_from_predicted'][X_test['income_bin'] == i]))\nactualByIncome = []\nfor i in range(10):\n    actualByIncome.append(X_test['loan_status'][X_test['income_bin'] == i].sum()/len(X_test['loan_status'][X_test['income_bin'] == i]))\n\n# table of predicted vs actual repayment rate by income bin\n\ndf = pd.DataFrame({'Income Bin': range(10), 'Predicted Repayment Rate': approvedByIncome, 'Actual Repayment Rate': actualByIncome})\nprint(df)\n\n# graph of predicted vs actual repayment rate by income bin\n\nplt.bar(range(10), approvedByIncome, color = 'red')\nplt.bar(range(10), actualByIncome, color = 'blue')\nplt.xlabel('Income Bin')\nplt.ylabel('Predicted vs actual repayment rate')\nplt.legend(['Approved', 'Actual'])\nplt.show()\n\n   Income Bin  Predicted Repayment Rate  Actual Repayment Rate\n0           0                  0.884692               0.516899\n1           1                  0.874751               0.604374\n2           2                  0.911439               0.789668\n3           3                  0.937093               0.728850\n4           4                  0.964215               0.807157\n5           5                  0.966135               0.824701\n6           6                  0.984344               0.835616\n7           7                  0.979920               0.877510\n8           8                  0.997984               0.893145\n9           9                  1.000000               0.904573\n\n\n\n\n\n\n\n\n\nI am so proud of my model. It does a very good job of not discriminating based on income level, with lower level incomes actually having a larger disparity between accepted and defaulted loans than higher income levels. Interestingly, the model predicts that the different income levels have a somewhat similar default rate, while the actual default rate is much higher for lower income levels. This is bad from a money perspective, but good from a moral perspective, as it shows that the model is not discriminating based on income level. Additionally, while one could argue that money is being lost to the poorer individuals, this is still maximizing profit with the current threshold. So, I suppose it could be seen as profitable to be charitable, at least in this case.\n\n\nReflection\nI was surprised by the level of relative fairness achieved by my model. It was able to maximize profit while not obviously discriminating based on age or income level. It is at least subtle, which is more than I can say for most people. I learned that thresholds are important in decision making, and that they can be used to determine the ‘optimal’ decision beyond predicting labels. I also learned that it is important to consider the ethical implications of decision making, and that profit is not the only consideration when making decisions.\nIn terms of further ethical considerations, medical expenses have high default rates. This may be due to the nature that those in medical debt may not recover, and thus may not be able to repay the loan. It bring us to the question of whether it is fair to make these loans more difficult to obtain. While I feel my model was relatively successful in not discriminating against those seeking medical loans, it is still important to consider the human cost of denying a single one of these loans. Even cosmetic surgeries can be life changing for some! In the end, we must decide what is fair.\nFairness is a two part decision here, as we must decide what is means to be fair as well as apply that definition to this particular instance. I believe that fair decision making means not making decisions on things which cannot be determined by the individual. Even though we live in a “pull yourself up by your bootstraps” society, most individuals are a product of their environments, and some boots don’t even have straps anymore! Further, many things are impacted by a combination of effort and luck. However, if we can determine what it means to have something be outside of an individual’s control, we can make reasonably fair decisions.\nWe must also consider the business cost of accepting medical loans. While humans are not numbers, the individuals who make up a business are. While profits in the current setting generally go straight to the top and most certainly do not drop in the fashion of trickle down economics, in a world where they did we would have to consider the cost of accepting these loans on those who give them out. On the flip side, an individual who is able to pay off a medical loan is likely to be a loyal customer in the future.\nIn conclusion, it is not fair for medical expenses to have higher rates of default than other loans, especially when old age, which also has a high rate of default via death, is not discriminated against. The most fair decision would be to de-profitize the medical industry, but that is a discussion for another day."
  },
  {
    "objectID": "posts/logistic regression/Implementing Logistic Regression.html",
    "href": "posts/logistic regression/Implementing Logistic Regression.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "# %load_ext autoreload\n# %autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\nCSCI 0451: Implementing Logistic Regression\n\nAbstract\nThis is an implementation of Logistic Regression. In it, I explore the mathematics behind the algorithm and implement it in Python. I then test the algorithm on a dataset. I explore concepts such as momentum and overfitting using the algorithm.\nMy logistic.py code can be found at https://github.com/jnerenberg/jnerenberg.github.io/blob/main/posts/logistic%20regression/logistic.py.\n\n\nExperimental Data\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n# graph the data\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color = 'red')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color = 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHow to Train Your Dragon (Model)\n\n\nimport torch.nn as nn\n\n# Get the number of features\ninput_dim = X.shape[1]\n\n# Initialize the Logistic Regression model and the optimizer\nLR = LogisticRegression(input_dim)  # Assuming binary classification\nlearning_rate = 0.01\nopt = GradientDescentOptimizer(LR, learning_rate)\n\n# Define the loss function\nloss_function = nn.BCELoss()\n\nfor _ in range(100):\n    # Compute the loss\n    y_pred = LR(X)\n    y = y.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y)\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\n\nExperiment 1:\nVanilla gradient descent: When the number of features , when is sufficiently small and , gradient descent for logistic regression converges to a weight vector that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations).\n\n# Flatten y to a 1D tensor\ny_flat = y.flatten()\n\n# Plot the decision boundary\nx1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\nweights = LR.linear.weight.detach().numpy()  # Convert the weights to a numpy array\nx2 = -(weights[0][0] * x1 + weights[0][2]) / weights[0][1]\n\nplt.plot(x1, x2, color = 'black')\nplt.scatter(X[y_flat == 0][:, 0], X[y_flat == 0][:, 1], color = 'red')\nplt.scatter(X[y_flat == 1][:, 0], X[y_flat == 1][:, 1], color = 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# print the accuracy\ny_pred = LR(X)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y, y_pred.detach().numpy())\nprint(f'Accuracy: {accuracy}')\n\n# do the losses converge?\nlosses = []\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = LR(X)\n    y = y.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y)\n    losses.append(loss.item())\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\nplt.plot(losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration')\nplt.show()\n\n\nAccuracy: 0.92\n\n\n\n\n\n\n\n\n\nThis looks like a decent linear decision boundary. It has an accuracy of 0.94, which I am happy with. It also looks similar to the line I would have chosen if I were to draw it by hand. The loss decreases with each iteration, which is also what I would expect, even though its a little slow.\n\n\nExperiment 2:\nBenefits of momentum: On the same data, gradient descent with momentum (e.g. ) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with ). Plot the loss over iterations for each method. You may need to experiment with the data and choice of in order to observe speedups due to momentum.\n\n# Benefits of momentum: On the same data, gradient descent with momentum (e.g. ) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with ). Plot the loss over iterations for each method. You may need to experiment with the data and choice of in order to observe speedups due to momentum.\n\n# momentum\nw2 = torch.randn(input_dim, 1, requires_grad = True)\nv = torch.zeros(input_dim, 1)\ngamma = 0.9\nlearning_rate = 0.01\nlosses = []\n\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = X @ w2\n    y_pred = torch.sigmoid(y_pred)\n    loss = loss_function(y_pred, y.view(y_pred.shape))\n    losses.append(loss.item())\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    with torch.no_grad():\n        v = gamma * v + learning_rate * w2.grad\n        w2 -= v\n    \n    # Zero the gradients\n    w2.grad.zero_()\n\nplt.plot(losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration')\nplt.show()\n\n\n\n\n\n\n\n\n\n# print the accuracy\ny_pred = LR(X)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y, y_pred.detach().numpy())\nprint(f'Accuracy: {accuracy}')\n\nAccuracy: 0.9166666666666666\n\n\nWoah! This converges much faster than the vanilla gradient descent. The loss decreases much faster and the accuracy is pretty much the same. It took about a fifth of the iterations to converge to the same accuracy. This is a huge improvement.\n\n\nExperiment 3:\nOverfitting: Generate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\n\n# Overfitting: Generate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\nX_val, y_val = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\n\n# Initialize the Logistic Regression model and the optimizer\ninput_dim = X_train.shape[1]\nLR = LogisticRegression(input_dim)  # Assuming binary classification\nlearning_rate = 0.01\nopt = GradientDescentOptimizer(LR, learning_rate)\n\n# Define the loss function\nloss_function = nn.BCELoss()\n\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = LR(X_train)\n    y_train = y_train.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y_train)\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\n# print the accuracy\ny_pred = LR(X_train)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_train, y_pred.detach().numpy())\nprint(f'Training accuracy: {accuracy}')\n\n# print the accuracy\ny_pred = LR(X_val)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_val, y_pred.detach().numpy())\nprint(f'Validation accuracy: {accuracy}')\n\n# print the accuracy\ny_pred = LR(X_test)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_test, y_pred.detach().numpy())\nprint(f'Test accuracy: {accuracy}')\n\nTraining accuracy: 1.0\nValidation accuracy: 0.98\nTest accuracy: 0.88\n\n\nTraining accuracy is 1.0, which is expected since we have more features than data points. The test accuracy is 0.88, which is honestly better than I expected, but still overfitted.\n\n\n\nConclusion\nIn conclusion, logistic regression is a powerful tool for binary classification. It is daunting yet rewarding to implement from scratch. I learned a lot about the math behind it and how to optimize it. I also learned about the importance of momentum in optimization. I am happy to better understand the tools we have been using and expect it will drastically improve my usage of the tool. Also, I am very confused by my desire to say “overfat” instead of “overfitted” when using the past tense of “overfit”. Perhaps I am overfitting some other language rule to this word."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog + some extra text yay"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nIn this blog post, I’ll implement a generalized form of gradient descent for logistic regression. \n\n\n\n\n\nMay 5, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nIn this blog post, I will further study optimal decision-making in the context of the the credit-risk prediction problem. My aim is to (a)determine a score function and threshold that maximize profit for the bank under more realistic assumptions and (b) assess which populations of prospective borrowers are most impacted by my proposed policy. \n\n\n\n\n\nMay 5, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nIn this blog post, I’ll work through a complete example of the standard machine learning workflow. Your primary goal is to determine the smallest number of measurements necessary to confidently determine the species of a penguin. \n\n\n\n\n\nApr 25, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nWomen in Data Science Conference\n\n\n\n\n\nThis blog post involves attending, reporting on, and reflecting on the Women in Data Science (WiDS) Conference at Middlebury College on March 4th, 2024. \n\n\n\n\n\nMar 4, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nReflective Goal-Setting\n\n\n\n\n\nWe plan our goals for learning, engagement, and achievement over the course of the semester! \n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html",
    "href": "posts/goal-setting-lol/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Julia Nerenberg\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am most interested in the Implementation and Theory aspects of Machine Learning. I already took a course at UMass Boston called “Ethics in Computing,” so I feel like I have the Social Responsibility section somewhat covered already.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI want to complete at least 5 blog posts. I want to make sure that I am able to write about the topics that I am most interested in, so I will focus on completing those well before stressing about completing the rest. I haven’t had many classes that have required me to critically discuss recent readings, so I am excited to have an opportunity to argue, which can be a lot of fun.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI want to show up to every class, baring any unexpected emergencies. I also want to show up with a strong attempt at the warm up activity. Sometimes, I’ll be somewhat absent from discussions if I don’t feel that I’ve adequately prepared, even if I have points to add. To combat this, I will always have at least some points prepared to bring up, so I don’t feel useless. For instance, today, I had a hard time thinking of a point to bring up, but I wrote down something I didn’t feel super confident about, and my group leader ended up mentioning it to the class. I also want to focus on this because having different perspectives is important, and I shouldn’t dismiss my ideas even if I dont initially feel they fit the mold given by the prompt.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI want to do a project on something I wouldn’t pick for myself. For instance, in my J-term class, I randomly chose the topic of my final project and ended up having a blast with it! I feel it will allow me to develop new opinions rather than just going into a topic that I already feel I know about. As a result, I would be happy with literally any topic. In terms of the group, Zoe would like to complete a cancer project, and I think she would like to do this on her own, but I have designated myself as her “back-up” group member if you won’t let her, so if you could get an answer to her on that so I can plan one way or another that would be wonderful. If I have to choose a topic, I would be intersted in an algorithm that identifies the most relevant personal information for sale for a given prompt. For instance, if I am stalking an individual and want to know their place of employment, I would want to purchase information regarding their location during work hours and maybe work associated emails. It would also be cool to have something that would pick up information in an image that can indicate where it was taken, like street signs, store signs, license plates’ state of origin, maybe even location of sun with respect to time and timezones. I am not a stalker. I just think it’d be neat."
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html#what-youll-learn",
    "href": "posts/goal-setting-lol/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am most interested in the Implementation and Theory aspects of Machine Learning. I already took a course at UMass Boston called “Ethics in Computing,” so I feel like I have the Social Responsibility section somewhat covered already."
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html#what-youll-achieve",
    "href": "posts/goal-setting-lol/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI want to complete at least 5 blog posts. I want to make sure that I am able to write about the topics that I am most interested in, so I will focus on completing those well before stressing about completing the rest. I haven’t had many classes that have required me to critically discuss recent readings, so I am excited to have an opportunity to argue, which can be a lot of fun.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI want to show up to every class, baring any unexpected emergencies. I also want to show up with a strong attempt at the warm up activity. Sometimes, I’ll be somewhat absent from discussions if I don’t feel that I’ve adequately prepared, even if I have points to add. To combat this, I will always have at least some points prepared to bring up, so I don’t feel useless. For instance, today, I had a hard time thinking of a point to bring up, but I wrote down something I didn’t feel super confident about, and my group leader ended up mentioning it to the class. I also want to focus on this because having different perspectives is important, and I shouldn’t dismiss my ideas even if I dont initially feel they fit the mold given by the prompt.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI want to do a project on something I wouldn’t pick for myself. For instance, in my J-term class, I randomly chose the topic of my final project and ended up having a blast with it! I feel it will allow me to develop new opinions rather than just going into a topic that I already feel I know about. As a result, I would be happy with literally any topic. In terms of the group, Zoe would like to complete a cancer project, and I think she would like to do this on her own, but I have designated myself as her “back-up” group member if you won’t let her, so if you could get an answer to her on that so I can plan one way or another that would be wonderful. If I have to choose a topic, I would be intersted in an algorithm that identifies the most relevant personal information for sale for a given prompt. For instance, if I am stalking an individual and want to know their place of employment, I would want to purchase information regarding their location during work hours and maybe work associated emails. It would also be cool to have something that would pick up information in an image that can indicate where it was taken, like street signs, store signs, license plates’ state of origin, maybe even location of sun with respect to time and timezones. I am not a stalker. I just think it’d be neat."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer penguins/Classifying-Palmer-Penguins.html",
    "href": "posts/palmer penguins/Classifying-Palmer-Penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "CSCI 0451: Classifying Palmer Penguins\n\nAbstract\nThis post is an exploration of the Palmer Penguins dataset. The dataset contains measurements of penguins from three different species. I will use the dataset to train a machine learning model to classify the species of penguins based on certain measurements. I will primarily be utilizing the pandas library, but I will also make use of the scikit-learn library to train a support vector machine (SVM) model on the dataset and the matplotlib library to visualize the data and the decision boundaries of the model.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nTime to clean the data.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nData Exploration\nHere are some exploratory visualizations of the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualization 1\n# Pairplot of the data, colored by species\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nsns.pairplot(train, hue = \"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nThis gives us a great overview of the data at a glance, but let’s take a closer look at some of the features that appear to be the most divisive between species. Seeing as the flipper length does a good job of separating the Gentoo penguins from the other two species, let’s select that at one of our initial features. We also want to look at a metric seems to isolate the two other species. Using the charts on the diagonal, we can see that delta 13 C is good for separating the Chinstraps, and the culmen length is good for separating the Adelies.\n\n# Visualization 2\n# Let's look more at the flipper length, delta 13 c, and culmen length\n# flipper length by species\nsns.boxplot(data = train, x = \"Species\", y = \"Flipper Length (mm)\")\nplt.xticks(rotation = 45)\nplt.title(\"Flipper Length by Species\")\nplt.show()\n# delta 13 c by species\nsns.boxplot(data = train, x = \"Species\", y = \"Delta 13 C (o/oo)\")\nplt.xticks(rotation = 45)\nplt.title(\"Delta 13 C by Species\")\nplt.show()\n# culmen length by species\nsns.boxplot(data = train, x = \"Species\", y = \"Culmen Length (mm)\")\nplt.xticks(rotation = 45)\nplt.title(\"Culmen Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, its time for some tables. We can use the groupby function to get the mean and median values of the flipper length, delta 13 C, and culmen length for each species. This will give us a good idea of the average values for each species, and will help us to see how the species differ in terms of these features.\n\n# Mean values\nmeans = train.groupby(\"Species\")[[\"Flipper Length (mm)\", \"Delta 13 C (o/oo)\", \"Culmen Length (mm)\"]].mean()\nprint(\"Mean values\")\nprint(means)\n\nMean values\n                                           Flipper Length (mm)  \\\nSpecies                                                          \nAdelie Penguin (Pygoscelis adeliae)                 190.084034   \nChinstrap penguin (Pygoscelis antarctica)           196.000000   \nGentoo penguin (Pygoscelis papua)                   216.752577   \n\n                                           Delta 13 C (o/oo)  \\\nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)               -25.796897   \nChinstrap penguin (Pygoscelis antarctica)         -24.553401   \nGentoo penguin (Pygoscelis papua)                 -26.149389   \n\n                                           Culmen Length (mm)  \nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                 38.970588  \nChinstrap penguin (Pygoscelis antarctica)           48.826316  \nGentoo penguin (Pygoscelis papua)                   47.073196  \n\n\n\n# Median values\nmedians = train.groupby(\"Species\")[[\"Flipper Length (mm)\", \"Delta 13 C (o/oo)\", \"Culmen Length (mm)\"]].median()\nprint(\"Median values\")\nprint(medians)\n\nMedian values\n                                           Flipper Length (mm)  \\\nSpecies                                                          \nAdelie Penguin (Pygoscelis adeliae)                      190.0   \nChinstrap penguin (Pygoscelis antarctica)                196.0   \nGentoo penguin (Pygoscelis papua)                        216.0   \n\n                                           Delta 13 C (o/oo)  \\\nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                -25.89709   \nChinstrap penguin (Pygoscelis antarctica)          -24.59467   \nGentoo penguin (Pygoscelis papua)                  -26.20455   \n\n                                           Culmen Length (mm)  \nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                      38.9  \nChinstrap penguin (Pygoscelis antarctica)                49.3  \nGentoo penguin (Pygoscelis papua)                        46.5  \n\n\nWhile these metrics are a great start, it is important to remember that human decision making is not always ideal or feasible. We can instead use some machine learning tools to help us determine the best features to use.\nLet’s perform an exhaustive search over all possible feature combinations to determine the best features to use for our model. We will use the combinations function from the itertools package for this.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Define the model\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 0)\n\n# Define the features\nfeatures = X_train.columns\n\n# Define the number of features to select\nn_features = 3\n\n# Perform the search\nbest_score = 0\nbest_features = []\nfor combo in combinations(features, n_features):\n  X_train_subset = X_train[list(combo)]\n  score = cross_val_score(model, X_train_subset, y_train, cv = 5).mean()\n  if score &gt; best_score:\n    best_score = score\n    best_features = combo\n\nprint(\"Best features: \", best_features)\nprint(\"Best score: \", best_score)\n\nBest features:  ('Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_MALE')\nBest score:  0.9844645550527904\n\n\nWow! That took a long time (over a minute!). It’s important to note that this method will not work for larger datasets, as the number of possible combinations grows exponentially with the number of features. However, for this dataset, it is a feasible method.\nTaking a look at the results, we can see that the best features to use are Culmen Length, Culmen Depth, and Sex. Time to train the model.\n\n\nModel Time\nTo start, lets define our Logistic Regression model. We will use the LogisticRegression class from the scikit-learn library to do this.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Define the model\nmodel = LogisticRegression(max_iter = 1000)\n\n# Define the features\nfeatures = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Sex_MALE\", \"Sex_FEMALE\"]\n\n# Train the model\nmodel.fit(X_train[features], y_train)\nprint(\"Train Set Score: \", model.score(X_train[features], y_train))\n\n# Let's now evaluate the model on the test set.\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nprint(\"Test Set Score: \", model.score(X_test[features], y_test))\n\n\nTrain Set Score:  0.9921875\nTest Set Score:  0.9852941176470589\n\n\nSuccess! Our model has an accuracy of 98% on the test data. This is a great result, but it is important to remember that this is a small dataset, and the model may not perform as well on larger datasets. However, for this dataset, the model is performing very well.\nNext up, let’s visualize the decision boundaries of the model.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Ensure axarr is a list\n    if not isinstance(axarr, np.ndarray):\n        axarr = np.array([axarr])\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(model, X_train[features], y_train)\n\n\n\n\n\n\n\n\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_pred = model.predict(X_test[features])\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, fmt = \"d\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\n\n\n\n\n\n\nThe above confusion matrix indicates that we have pretty great results. We are mostly getting the correct species, with only a few misclassifications. We can see from the graph that the misclassifications are mostly due to special penguin specimens who have outlying features. This is good because it means that our model is likely not overfitting to the training data. Lets verify this using cross-validation.\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X_train, y_train, cv = 5)\nscores.mean()\nprint(\"Cross Validation Score: \", scores.mean())\n\nCross Validation Score:  1.0\n\n\n\n\nDiscussion\nIn this post, I explored the differences between the three species of penguins in the Palmer Penguins dataset. I used the dataset to train a machine learning model to classify the species of penguins based on certain measurements. Some of the measurements were quantitative while others were qualitative. I was able to use the combinations function from the itertools package to determine the best features to use for the model. I then trained a logistic regression model from the scikit-learn library on the dataset and visualized the decision boundaries of the model. The model was highly accurate and performed well on the test data. I also used cross-validation to verify that the model was not overfitting to the training data. Overall, the model performed very well on the dataset, and the results were promising."
  }
]