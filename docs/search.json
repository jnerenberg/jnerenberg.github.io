[
  {
    "objectID": "posts/women in data science conference/Women-in-Data-Science.html",
    "href": "posts/women in data science conference/Women-in-Data-Science.html",
    "title": "Women in Data Science Conference",
    "section": "",
    "text": "CSCI 0451: Women in Data Science Conference\n\nAbstract\nThis blog post is a summary of and reflection of the Women in Data Science Conference that I attended on March 4, 2024. The conference was held at Middlebury College and was organized by Breanna Guo, a senior. The conference featured several speakers, including keynote speaker Sarah Brown, a Computer Science professor at the University of Rhode Island. There were many familiar faces at the conference, including several of my classmates and professors. It was a great opportunity to learn more about the field of data science and to connect with my professors and peers.\n\n\nWhy Spotlight Women in Data Science?\nUnderrepresentation of women in computing, math, and engineering contributes to a significant loss of talent and innovation in these fields. The lack of diversity in these fields is a problem because it limits the perspectives and ideas to those centered around the male demographic. This is a problem for everyone, as it means that the products and services that are created are not as inclusive or as innovative as they could be.\nThe representation and status of women in computing today differs from the 1950s and 1960s in that it has gone from a role that was predominantly filled by women to one that is predominantly filled by men. Initially, computing positions were seen as low-status, low-paying jobs that were considered to be “women’s work.” In a way, it was seen as similar to secretary work. However, as the field of computing grew and became more visible, men began to be favored for these positions as they were often seen as more competent and capable. The advent of the personal computer also played a role in this shift, as it was marketed towards gamers and the “anti-social nerd” male stereotype. This led to men being more likely to pursue careers in computing, having been encouraged by this stereotype and exposed to computers at a young age.\nMany of the barriers and unequal challenges attributed to the underrepresentation of women in computing can be eroded by events that spotlight the achievement of women in STEM. One of the main barriers is the lack of female role models and mentors in these fields. The achievements of the modern women in computer can serve to inspire and motivate young women to pursue careers in computing. Additionally, the stereotypes barring women from these fields can be broken down by showcasing their accomplishments. It is a compounding success, as the more women are spotlighted in these fields, the more likely it is that women will be encouraged to pursue careers in computing, and the more likely it is that the stereotypes will be broken down.\n\n\nThe Conference\nThe first lightning talk of the conference was Professor Amy Yuen, a Political Science professor at Middlebury College. Professor Yuen spoke about the importance of data science in the field of political science. She explained her work in using data science to analyze the United Nation’s Security Council membership changes over time. With only 15 members on the UN Security Council and 5 of which being permanent members, sponsorship plays a significant role in the election of non-permanent members. Professor Yuen argued that while this method may seem un-representative, the seats that are campaigned for general have somewhat equitable representation. This application of data science in the political sector is integral to understanding the motives of the UN and the power dynamics shaping our world.\nThe next talk was by the keynote speaker, Professor Sarah Brown, a Computer Science professor at the University of Rhode Island. Professor Brown spoke about her experiences in the field of data science as a multi-disciplinary field. She argued that data science is not just about the data, but about the people who are using the data. She emphasized the importance of collaboration between data scientists and domain experts, as well as the importance of communication skills in data science. Professor Brown also discussed diversity in the field, as it leads to more innovative and inclusive solutions. I learned that data science is not just about the data, but about the people who are using the data. I also learned that collaboration and communication are key skills in data science, and that diversity is important for creating innovative and inclusive solutions.\nProfessor Brown’s presentation was organized around three keys: contextualize data, disciplines are communities, and meet people where they are. She explained how data is a primary source of information and needs to be examined with a critical eye, much like how one would in a social studies environment. She also expressed the importance of seeking the expertise of individuals rooted within the field of study, as they can provide valuable insights that data alone cannot. Lastly, she emphasized the importance of meeting people where they are. She discussed her experiences on the board of the National Society for Black Engineers and how she realized the barriers between policy change at the higher levels and policy enaction at the lower levels. These keys, while each seeming to be a small piece of the puzzle, form a strong basis for fair and effective data science practices.\nProfessor Brown’s talk was followed by a lightning talk by Professor Jessica L’Roe, a professor of geography at Middlebury college. Her talk was focused on her experiences in using data science to analyze the usage of farmland around the world. She focused on the impact of shifts in farmland usage on local communities. She found that shifts in farmland ownership from indigenous communities to large corporations cause many families to encourage their youth to leave the farming industry. Another interesting find by Professor L’Roe concerned the reporting of farmland area owned by entities. She found that there was a much larger concentration of farms approaching the upper limits of regulations than the lower limits. While this could be a result of purchasing land in specific increments, it most likely suggests that there is a significant amount of underreporting of farmland ownership by those who own more land. Professor L’Roe’s work was a great example of how data science can be used to analyze and understand complex socio-economic and environmental issues.\nThe final talk of the conference was by Professor Laura Biester, a professor of computer science at Middlebury College. Professor Biester spoke primarily about her work in the natural language processing field, and shared some of her research on emotion detection in text posts. Amazingly, she was able to compile a dataset of almost every reddit post. Using this data, she was able to work on detection and prediction of depression in text posts. In order to ethically do this, Professor Biester relied on self reported diagnoses of depression within the dataset. Interestingly, despite the fact that these self reported diagnoses could be fabricated, those reports are few and far between, and had no significant impact on the study. Professor Biester’s work concluded by being able to somewhat accurately predict depression diagnoses from textual features in written works. In the future, this sort of work can be used to help identify individuals who may be at risk for developing depression and provide them with early intervention.\n\n\nReflection\nI was inspired by the WiDS conference. It reminded me of my reasons for choosing Computer Science as a major: it is applicable to any field. Often, computer science is seen as a field that is separate from other fields, but the WiDS conference demonstrated its unique ability to bridge the gaps between disciplines. I have always considered myself to be a multi-disciplinary person, and value adding skillsets to my toolbox. Data and computer science are the tools that facilitate this goal. In particular, it was enlightening to see the work of Professor Biester outside of the classroom. As a student of hers, I was struck by the realization that her work could one day directly benefit someone like me. It was a reminder that the work we do in the classroom is not just for the sake of learning, but for the sake of making a difference in the world. I was also stunned by how close we as students of computer science are to being able to do some of this same work. The tools and techniques that we are learning in our classes are the same ones that are being used by professionals in the field; a great reminder that we are not just students, but future professionals."
  },
  {
    "objectID": "posts/Project Blog Post/ProjectBlogPost.html",
    "href": "posts/Project Blog Post/ProjectBlogPost.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "Project Blog Post\n\nOn skin cancer identification using CNN and Logistic Regresison models\n\n\nAbstract\nSkin cancer is the most common form of cancer. According to the Victoria department of health, over 95% of skin cancers that are dected early can be successfully treated. This means that early detection is a crucial step in the treatment process of skin cancer. In our analyses, we aim to identify different types of skin conditions with image classification techniques. Using convolutional neural networks (CNN) and logistic regression (LR) models, we obtain a 75% accuracy rate for classification, where melanocytic nevi (NV) and melanoma (MEL) are the most successfully classified forms of skin condition.\nLink to GitHub Repository: https://github.com/ShadowedSpace/cancer\n\n\nIntroduction\nIn 1994, Binder et al. trained a neural network that successfully differentiated between melanomas, the leading cause of skin cancer deaths, and melanocytic nevus, which are generally harmless skin lesions commonly known as moles or birth marks. In 2018, a challenge hosted by the International Skin Imaging Collaboration (ISIC) lay out the task of detection and classification of skin lesions and diseases. 900 parties registered to download the data for the challenge and hundreds of groups submitted novel evaluation techniques to automate the process of diagnosing skin lesions and diseases. A study conducted on the ISIC challenge revealved that the best classification models still failed to properly classify on average over 10% of dermoscopic images and had varying abilities to generalize (Coedlla et al., 2019). As it turns out, Binder et al.’s study (1994) trained their neural network on 200 images. The dataset used by the ISIC challenge is the world’s largest public repository of dermoscopic images of skin and contains 10015 dermoscopic images. First released in 2018 by Philipp Tschandl et al., the HAM10000 (Humans Against Machines) is a novel dataset with the aim of improving the process of automated skin lesion diagnosis, as most existing demoscopic image datasets are either small in size or lacking diversity.\nAccording to a recent study by Tandon et al. (2024), the most successful deep learning model to implement for the automation of cancer diagnoses is the convolutional neural network (CNN). CNNs are a type of neural network that are particularly effective for identifying patterns in images and audio. CNNs are best used when there are large amounts of data to train the model on (MathWorks). Before the release of the HAM10000 dataset, the diversity and size of dermatoscopic images was limited, which also limited the progression of automated skin lesion detection (Tschandl et al. 2024). Binder et al.(1994) creates a model from a much smaller sample size and is limited in the diversity of skin lesions it can detect. As revealed by the 2018 ISIC challenge, automated healthcare diagnoses are still limited in accuracy and generalizability.\nIn our project, we apply our knowledge of Convolutional Neural Networks (CNN) and other machine learning techniques to the HAM10000 dataset with the goal of classifying types of skin lesions. Accurate and early skin lesion diagnosis is crucial to a successful recovery when it comes to skin cancer. Melanoma, a type of skin cancer, is quick to spread to other organs, and is the leading cause of deaths due to skin cancer. But when detected early enough, before the melanoma has a chance to spread, melanoma has a five-year survival rate of 98%. Precancerous or skin cancers spots can be spotted by the naked eye. According to the Fred Hutch Center, the ABCDE Guice is a way to visually detect abnormalities in skin lesions:\nA for asymmetry- early melanomas are asymmetrical while moles are symmetrical\nB for border- early melanomas tend to have uneven borders\nC for color- early melanomas are often a vareity of shades of brown while common moles tend to be one shade\nD for diameter- the diameter of melanomas are typically larger than that of a mole\nE for evolution- changes in a skin lesion may indicate skin cancer\nAs seen in Figure 1, the majority of skin lesions in the HAM10000 dataset are melanocytic nevi (NV), and the second most common skin lesions are melanomas (MEL).\nWe use CNN and logistic regression models to classify types of skin lesions identified in the HAM10000 dataset. Using the logistic regression models on non-image variables provided in the dataset, we achieve a testing accuracy of 70%. Including images when training the model does not significantly increase the accuracy rate. When using the CNN method, we apply class weights as the dataset is highly imbalance towards NV diagnoses. This results in a model with 75% testing accuracy.\nWe also separate skin lesion types into ‘malignant’ and ‘benign’ categories. When training to classify more dangerous skin lesions from harmless skin lesions, the weighted logistic regression model is able to correctly identify 57% of demoscopic images showing malignant skin lesions. Compared to the CNN model of simply classifying skin lesions, where malignant skin lesions are correctly identified 3% to 16% of the time, grouping the malignant skin lesions for classification results in a much improved model for correctly indentifying malignant skin lesions.\nWhile the imbalanced dataset made the methods we used more difficult to train to achieve higher accuracy than baseline, weighing these models achieves an accuracy of classification of 75%, which is about 10 percentage points greater than the baseline accuracy. Additionally, our findings from malignant skin lesion identification reveal that combining malignant skin lesion categories for classification significantly improves the model performace.\n\n\nValues Statement\nWe created this model as a way to explore how technologies like convolutional neural networks could be used on a real-world problem. In the state it is in now, we do not recommend using our model as a true diagnosis tool. However, if we were to continue to improve this model to higher accuracy, there is a world where community members could upload photos of skin lesions and receive a percentage risk score that their skin lesion might benefit from analysis from a professional. That being said, an image would need to be high-quality and under professional lighting to match the image style of the images in HAM10000.\nOne of the greatest challenges in the healthcare industry is access. Geographical “deserts” exist all across the country where population healthcare needs are unmet partially or totally due to lack of adequate access (Brinzac et al). TeleHealth and WebHealth solutions are aiding these communities; a skin lesion classification algorithm could be used in such services.\nThe HAM10000 dataset was created to address the issue of small size and lack of diversity in publically available skin cancer datasets. That being said, the images were collected from the Austrian and Australian population, consisting of predominantly white individuals. We have not tested how our algorithm’s results when applied to skin lesions on darker skin tones, but it would likely underperform.\nSpeak with just about anyone and it’s likely that they or somebody they love has been touched by cancer. This is certainly true for the four of us, especially Zoe who received a cancer diagnosis last spring. We combined Liz’s interest in applying machine learning to improve health outcomes with Zoe’s specific interest in cancer, and settled on the HAM10000 dataset because of its wide use.\nIt is our hope that the world would be a more joyful place with the implementation of this algorithm. Beacuse of the crucial role of early recognition in skin cancer diagnosis, we hope that this model could allow people to more readily determine if they should get a skin lesion evaluated by a medical professional.\n\n\nMaterials and Methods\n\nData\nFor this project, we used the publicly available HAM10000 dataset, found on the Harvard Dataverse (Tschandl, 2023). It includes 10015 dermatoscopic images of skin lesions which were collected over a period of 20 years at two different sites, one in Austria and the other in Australia (Tschandl et al. 2018).\nAlong with the image data, we used the provided metadata file that contained information such as lesion_id, image_id, dx, dx_type, age, sex, localization, dataset. Some lesions were photographed multiple times, and dx refers to the diagnosis. One of the first challenges was converting the images into 2D numpy arrays and adding them into the data frame with the dx information (the target we were attempting to predict).\nThe limitations of this data were immediately obvious. This includes bias in the type of skin lesions represented in the data, in addition to bias of the demographics and skin types of patients with lesions represented in the data. As mentioned above, the data was collected in Australia and Austria, meaning that darker skin tones were not represented. This is particularly important to consider in a data set being used for machine learning, because if the machine doesn’t learn on a fair data set, there is likely to be a trickle down effect into the health care system, further systematizing racial injustices in patient treatment.\n\n\nFigure 1: Frequency of Skin Lesion Categories\n\n\n\nalt text\n\n\nThis figure shows the frequency of diagnoses present in the data set. Clearly, the nv dx dominates the space, bringing up questions about how well a model trained on this data will classify any non-nv diagnoses. This issue will be further discussed below in the approach section.\n\nApproach\nThe features of our data used to make predictions were the images, converted into 32x32x3 numpy arrays. The goal was to predict the column of the metadata table labeled dx, the diagnosis corresponding to each lesion.\nWe subset our data into two sets, 80% for the training set and 20% for the test set. This train-test split provided a large enough sample to properly train the model, while also leaving enough data unseen to evaluate whether overfitting occurred.\nTo start out, we used the sci kit learn LogisticRegression model. This was mostly a preliminary effort to eliminate any bugs and see how we were doing in comparison to the base accuracy. After this, we moved on to our main model, which was a convolutional neural network (CNN). This decision was primarily based off previously published papers showing that this method was the most successful. We attempted several variations, each with and without data augmentation. First, we tried a minimally connected linear model. Then we added additional ReLU and Conv2D layers to see if the additional layers would improve the accuracy. We also attempted a model with added dropout layers, and a dropout probability of 25%. We started to be suspicious of the results when a model recommended by a published paper was not yielding good accuracy. After investigating this issue, we realized that because of the high prevalence of the dx nv in our dataset, the model was always choosing this as the correct answer. To fix this, we implemented class weights and transfer learning. Transfer learning proved more successful, yielding our best accuracy of around 75%.\nOriginally, we trained our models on 10 or 20 epochs. Once we isolated the best models, we spent the time to do a 100 epoch training period. All training was performed on our personal laptops.\nOur models were evaluated on accuracy of the validation set after extensive training. Confusion matrices were used extensively to investigae in which categories the misclassifications were occurring. As mentioned above, the training set contained 80% of the data (~8000 images) while the validation set was comprised of 20% (~2000 images).\n\n\n\n\nResults\nOur best models were the product of data augmentation, class weights, and transfer learning. First, we trained an ImageNet model, as designed by Stanford reserachers in 2009. Training for 100 epochs took about 240 minutes but produced an accuracy of about 70% on a validation set. The accuracy steadily increased over the epochs:\n\n\n\nTrainingGraph\n\n\nTake a look at the confusion matrix:\n\n\n\nConfusionMatrix\n\n\nOur model is best at predicting nv, or melanocytic nevi. Unfortunately, this is a benign skin lesion. It is the majority class in our model, explaining why it is predicted most often. That being said, we see better accuracy with melanoma and benign keratosis than previous models.\nWith the intention of increasing classification accuracy for melanoma, the most dangerous skin cancer, we utilized mobilenet_v2, Google’s transfer learning model from 2007. By adjusting the class weights to target the minority classes more aggressively, we were able to produce the following results on validation data.\n\n\n\nConfusionMatrix\n\n\nWe see increased accuracy for melanoma, at the expense of melanocytic nevi.\n\n\nConcluding Discussion\nYour conclusion is the right time to assess:\nIn what ways did our project work? Did we meet the goals that we set at the beginning of the project? How do our results compare to the results of others who have also studied similar problems? If we had more time, data, or computational resources, what might we do differently in order to improve further?\nOur project was less successful than we had initially hoped. Several of our members have been impacted by cancer outside of this project, so we were particularly motivated to create a model that could be used to help people. While our intentions and motivations were good, our results were not as successful as we had hoped. We were able to achieve a 70% accuracy rate, which is better than the baseline accuracy of 65%. However, this is still not a high enough accuracy rate to be used as an accurate diagnostic tool. We had hoped that our model would provide an accessible alternative to professional diagnosis, but it was instead only the first step in a long process.\nWhile we most certainly did not cure cancer, we met many of our exploration and implementation goals. Our model has some predictive power, and we were able to explore the HAM10000 dataset in depth. We were able to implement a convolutional neural network, which is a powerful tool for image classification and we were able to explore the limitations of our model and the dataset. We were also able to implement class weights and transfer learning to improve our model’s accuracy. These fully fulfilled our goals of utilizing machine learning techniques to classify skin lesions.\nOur results were within the range of similarly experienced projects using the HAM10000 dataset. However, one group of researchers was able to achieve an accuracy of 84.3% with an EfficientNet model, which is a more advanced model than the ones we used (Tajerian et al., 2023). Another group achieved an accuracy of 96.15 using Google’s ViT patch-32 model (Himel et al., 2024), which is also a more advanced model than the ones we used. Our results were not as successful as these groups, but they were within the range of other groups that used simpler models.\nIf we’d had more time and resources to continue this project, we likely would have explored some of the more advanced models mentioned above, as well as attempted to replicate some of their results. We were also limited by the size of the dataset, which, while relatively small for a machine learning project, is rather large to be processed on a personal laptop. We would have liked to further explore some of the ethical implications of our model, such as the potential for racial bias in the data and the potential for our model to be used as a diagnostic tool in underserved communities.\n\n\nGroup Contribution Statement\nOur group was formed because the topics proposed by Liz and Zoe had similarities regarding identification in the health space. Once we decided to look into cancer diagnoses, Zoe presented several dataset options to the group. We decided to focus on skin cancer identification and looked into datasets as a group, ultimately landing on the HAM10000 dataset. In doing the analyses, we decided to split the workload so that Liz and Zoe work primarily on the building the convolutional neural network, while Julia and Breanna worked on improving the logistic regression models. For the final blog post, Breanna wrote the abstract, introduction and group contribution statement. Julia summarized the concluding thoughts from the project. Liz wrote the values statement and worked on the results section jointly with Zoe. Zoe worked with Liz on the results and wrote the data/methods section.\n\n\nPersonal Reflection\nI learned a lot from the project. I think one of the most important things I learned here is that Machine Learning isn’t always the complete answer. It’s a tool that can be used to help solve problems, but it’s not a magic bullet. I also learned a lot about the importance of data quality and quantity. The HAM10000 dataset was a great dataset to work with, but it was limited in size and diversity. I think that if we had a larger and more diverse dataset, we could have achieved better results and further explored biases in our model. I also learned a lot about the importance of communication in group projects. I think that our group did a great job of communicating and working together, and it was fun sharing our skills from different CS and non-CS backgrounds.\nI wish we had been able to do more with ADA, as I’ve had a lot of fun using it in the past and it could have sped up some of our computations, but moving things over to ADA with only me having access to it already. I think that we did a good job of meeting our initial goals, but we fell short of our hopes. I think that we could have achieved better results if we had more time to examine and replicate other groups’ results, but I also think we did a good job at providing decent results outside of external influences.\nOverall, I became slightly less enthused about some aspects of the project when we didn’t get immediate or perfect results, but that was a good learning experience. I think that I will carry the experience of working on this project into my next problem solving expedition and attempt to apply the tools I’ve learned here to other problems. I think that I will also carry the experience of working in a group into my next courses and career stages, as while coding is often considered to be isolated work, this project has shown me how collaborative computer science as a field can be.\n\n\nSources\nBinder, M., A. Steiner, M. Schwarz, S. Knollmayer, K. Wolff, and H. Pehamberger. 1994. “Application of an Artificial Neural Network in Epiluminescence Microscopy Pattern Analysis of Pigmented Skin Lesions: A Pilot Study.” British Journal of Dermatology 130 (4): 460–65. https://doi.org/10.1111/j.1365-2133.1994.tb03378.x.\nCodella, Noel, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, et al. 2019. “Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC).” arXiv. https://doi.org/10.48550/arXiv.1902.03368.\nServices, Department of Health & Human. n.d. “Melanoma.” Department of Health & Human Services. Accessed May 13, 2024. http://www.betterhealth.vic.gov.au/health/conditionsandtreatments/melanoma.\nHimel, Galib Muhammad Shahriar et al. “Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System.” International journal of biomedical imaging vol. 2024 3022192. 3 Feb. 2024, doi:10.1155/2024/3022192. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10858797/.\n“Skin Cancer Early Detection.” n.d. Fred Hutch. Accessed May 13, 2024. https://www.fredhutch.org/en/patient-care/prevention/skin-cancer-early-detection.html.\nTajerian, Amin et al. “Design and validation of a new machine-learning-based diagnostic tool for the differentiation of dermatoscopic skin cancer images.” PloS one vol. 18,4 e0284437. 14 Apr. 2023, doi:10.1371/journal.pone.0284437. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10104315/.\nTandon, Ritu, Shweta Agrawal, Narendra Pal Singh Rathore, Abhinava K. Mishra, and Sanjiv Kumar Jain. 2024. “A Systematic Review on Deep Learning-Based Automated Cancer Diagnosis Models.” Journal of Cellular and Molecular Medicine 28 (6): e18144. https://doi.org/10.1111/jcmm.18144.\nTschandl, Philipp. 2023. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Harvard Dataverse. https://doi.org/10.7910/DVN/DBW86T.\nTschandl, Philipp, Cliff Rosendahl, and Harald Kittler. 2018. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Scientific Data 5 (1): 180161. https://doi.org/10.1038/sdata.2018.161.\n“What Is a Convolutional Neural Network? | 3 Things You Need to Know.” n.d. Accessed May 13, 2024. https://www.mathworks.com/discovery/convolutional-neural-network.html.\nBrinzac, Monica, Kuhlmann, Ellen, Dussault, Gilles. “Defining medical deserts – an international consensus-building exercise.” PubMed, National Center for Biotechnology Information. https://pubmed.ncbi.nlm.nih.gov/37421651/#:~:text=Results%3A%20The%20agreed%20definition%20highlight."
  },
  {
    "objectID": "posts/optimal decision making/'Optimal' Decision-Making.html",
    "href": "posts/optimal decision making/'Optimal' Decision-Making.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "CSCI 0451: ‘Optimal’ Decision Making\n\nIntroduction\nThis post explores the concept of ‘optimal’ decision making in the context of computer science. We will discuss the concept of ‘optimal’ and how it can affect the decision-making process. We will also explore some of the challenges and limitations of ‘optimal’ decision making in computer science.\nI use a logistic regression model to predict whether a loan will be paid off or not. I then calculate a threshold for the model to maximize profit. I then use this threshold to make decisions on whether to lend money to a borrower or not.\nTo spoil the ending, with money-minded decision making, we were able to profit almost $3000 per loan. However, this doesn’t take into account any of ethical considerations that come with not lending money. This is a very important consideration to make when making decisions in the real world, as many decisions are not as simple as maximizing profit.\nLet’s begin by grabbing the data:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\nExploring the Data\nThe first step is to look at the data: how it looks, what it contains, and initial observations. Lets take a look.\n\n# data exploration\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\ndf_train.describe()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\n\n\ncount\n26064.000000\n2.606400e+04\n25351.000000\n26064.000000\n23566.000000\n26064.000000\n26064.000000\n26064.000000\n\n\nmean\n27.734385\n6.597481e+04\n4.783559\n9569.468424\n11.009372\n0.217580\n0.169970\n5.794467\n\n\nstd\n6.362612\n6.330551e+04\n4.166275\n6297.660303\n3.246762\n0.412608\n0.106491\n4.055432\n\n\nmin\n20.000000\n4.000000e+03\n0.000000\n500.000000\n5.420000\n0.000000\n0.000000\n2.000000\n\n\n25%\n23.000000\n3.887250e+04\n2.000000\n5000.000000\n7.900000\n0.000000\n0.090000\n3.000000\n\n\n50%\n26.000000\n5.500000e+04\n4.000000\n8000.000000\n10.990000\n0.000000\n0.150000\n4.000000\n\n\n75%\n30.000000\n7.900000e+04\n7.000000\n12000.000000\n13.470000\n0.000000\n0.230000\n8.000000\n\n\nmax\n144.000000\n6.000000e+06\n123.000000\n35000.000000\n23.220000\n1.000000\n0.830000\n30.000000\n\n\n\n\n\n\n\nFirst off, I’m very interested in the maximum age being 144. For the sake of my waning amusement, lets take a closer look at them.\n\nprint(\"Entry with maximum person_age:\\n\", df_train.loc[df_train['person_age'].idxmax()])\n\nEntry with maximum person_age:\n person_age                        144\nperson_income                  250000\nperson_home_ownership            RENT\nperson_emp_length                 4.0\nloan_intent                   VENTURE\nloan_grade                          C\nloan_amnt                        4800\nloan_int_rate                   13.57\nloan_status                         0\nloan_percent_income              0.02\ncb_person_default_on_file           N\ncb_person_cred_hist_length          3\nName: 1561, dtype: object\n\n\nFascinating! Moving on, let’s make some visualizations to better understand the data.\n\n# let's make some visualizations to better understand the data\nimport matplotlib.pyplot as plt\n\n# histogram of person_age\nplt.hist(df_train['person_age'], bins = 20)\nplt.xlabel('Person Age')\nplt.ylabel('Frequency')\nplt.title('Histogram of Person Age')\nplt.show()\n\n# histogram of person_income\nplt.hist(df_train['person_income'], bins = 20)\nplt.xlabel('Person Income')\nplt.ylabel('Frequency')\nplt.title('Histogram of Person Income')\nplt.show()\n\n# histogram of loan_int_rate\nplt.hist(df_train['loan_int_rate'], bins = 20)\nplt.xlabel('Loan Interest Rate')\nplt.ylabel('Frequency')\nplt.title('Histogram of Loan Interest Rate')\nplt.show()\n\n# histogram of loan_percent_income\nplt.hist(df_train['loan_percent_income'], bins = 20)\nplt.xlabel('Loan Percent Income')\nplt.ylabel('Frequency')\nplt.title('Histogram of Loan Percent Income')\nplt.show()\n\n# histogram of cb_person_cred_hist_length\nplt.hist(df_train['cb_person_cred_hist_length'], bins = 20)\nplt.xlabel('Credit History Length')\nplt.ylabel('Frequency')\nplt.title('Histogram of Credit History Length')\nplt.show()\n\n# bar plot of loan_status\ndf_train['loan_status'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Status')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Status')\nplt.show()\n\n# bar plot of loan_intent\ndf_train['loan_intent'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Intent')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Intent')\nplt.show()\n\n# bar plot of person_home_ownership\ndf_train['person_home_ownership'].value_counts().plot(kind='bar')\nplt.xlabel('Home Ownership')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Home Ownership')\nplt.show()\n\n# bar plot of loan_grade\ndf_train['loan_grade'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Grade')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Grade')\nplt.show()\n\n# bar plot of cb_person_default_on_file\ndf_train['cb_person_default_on_file'].value_counts().plot(kind='bar')\nplt.xlabel('Default on File')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Default on File')\nplt.show()\n\n# bar plot of cb_person_cred_hist_length\ndf_train['cb_person_cred_hist_length'].value_counts().plot(kind='bar')\nplt.xlabel('Credit History Length')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Credit History Length')\nplt.show()\n\n# bar plot of person_emp_length\ndf_train['person_emp_length'].value_counts().plot(kind='bar')\nplt.xlabel('Employment Length')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Employment Length')\nplt.show()\n\n# bar plot of loan_amnt\ndf_train['loan_amnt'].value_counts().plot(kind='bar')\nplt.xlabel('Loan Amount')\nplt.ylabel('Frequency')\nplt.title('Bar Plot of Loan Amount')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems that a lot of the data is concentrated around the mean. This is a good sign, as it means that the data is not too spread out. This will make it easier to analyze and make decisions based on general trends.\nSince we are interested in determining the ‘optimal’ decision, we will need to define what ‘optimal’ means in this context. In the most simplified terms of loans, we want to minimize the risk of default while maximizing the return on investment. This is a classic trade-off problem in computer science, and we will need to balance these two objectives to make the ‘optimal’ decision. However, it is important to note that this is a gross generalization and not necessarily indicative of individual cases.\n\n\nBuilding a Model\nTo build a model, we will need to define the features and labels of the data. The features are the attributes of the data that we will use to make predictions, excluding loan_garde, while the labels are the outcomes that we are trying to predict. In this case, the features are the attributes of the loans, such as the loan amount, interest rate, and term length, while the labels are the outcomes of the loans, such as whether the loan was repaid or defaulted.\nWe will use a Logistic Regression model to do this. Lets try to train the model on all of the attributes excluding loan_grade and loan_status and see how well it performs.\n\n# We will use a Logistic Regression model to do this. Lets try to train the model on all of the attributes excluding loan_grade and loan_status and see how well it performs.\n\nfrom sklearn.linear_model import LogisticRegression\n\nX_train = df_train\n\n# drop loan_grade\nX_train = X_train.drop(['loan_grade'], axis = 1)\n\n# drop loan_status\nX_train = X_train.drop(['loan_status'], axis = 1)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\n1200\n9.91\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\n11750\n13.47\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\n10000\n7.51\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\n1325\n12.87\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\n15000\n9.63\n0.28\nN\n10\n\n\n\n\n\n\n\n\n# one-hot encode the categorical variables\nX_train = pd.get_dummies(X_train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n0\n25\n43200\nNaN\n1200\n9.91\n0.03\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n# encode true/false columns as 1/0\nX_train['person_home_ownership_MORTGAGE'] = X_train['person_home_ownership_MORTGAGE'].astype(int)\nX_train['person_home_ownership_OTHER'] = X_train['person_home_ownership_OTHER'].astype(int)\nX_train['person_home_ownership_OWN'] = X_train['person_home_ownership_OWN'].astype(int)\nX_train['person_home_ownership_RENT'] = X_train['person_home_ownership_RENT'].astype(int)\nX_train['loan_intent_DEBTCONSOLIDATION'] = X_train['loan_intent_DEBTCONSOLIDATION'].astype(int)\nX_train['loan_intent_EDUCATION'] = X_train['loan_intent_EDUCATION'].astype(int)\nX_train['loan_intent_HOMEIMPROVEMENT'] = X_train['loan_intent_HOMEIMPROVEMENT'].astype(int)\nX_train['loan_intent_MEDICAL'] = X_train['loan_intent_MEDICAL'].astype(int)\nX_train['loan_intent_PERSONAL'] = X_train['loan_intent_PERSONAL'].astype(int)\nX_train['loan_intent_VENTURE'] = X_train['loan_intent_VENTURE'].astype(int)\nX_train['cb_person_default_on_file_N'] = X_train['cb_person_default_on_file_N'].astype(int)\nX_train['cb_person_default_on_file_Y'] = X_train['cb_person_default_on_file_Y'].astype(int)\n\n# drop NAs\nX_train = X_train.dropna()\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n# train the model\n# Choose your features, estimate new ones if needed, and fit a score-based machine learning model to the data. My suggestion is LogisticRegression. Once you have fit a logistic regression model, the weight vector is stored as the attribute model.coef_.\nY_train = df_train['loan_status']\nY_train = Y_train[X_train.index]\n\nmodel = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, Y_train)\n\n# get the weights\nweights = model.coef_\nweights = weights[0]\nweights\n\narray([-6.36230941e-08, -4.05736686e-05, -2.49216289e-08,  1.06558109e-04,\n        9.49857325e-08,  2.54996456e-09, -1.22515188e-08, -6.57168949e-09,\n        5.82453376e-11, -3.93934560e-09,  8.07258494e-09,  2.68000657e-09,\n       -3.01882534e-09,  1.83914839e-09,  1.33270518e-09, -1.41426169e-09,\n       -3.79897794e-09, -9.65224646e-09,  7.27204165e-09])\n\n\nThe weights tell us how much each feature contributes to the prediction. The larger the weight, the more important the feature is in determining the outcome. We can use these weights to make predictions on new data and determine the ‘optimal’ decision.\n\n# test the model with cross-validation\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, Y_train, cv = 5)\n\narray([0.80379747, 0.81209079, 0.82558393, 0.80571928, 0.80921196])\n\n\nThe model seems to be performing well, with cross validation scores averaging to about 0.81. This is a good sign, but we need to now find a threshold to determine whether a loan should be approved or not. This is where the concept of ‘optimal’ decision making comes into play. There are two assumptions we should make to determine whether a loan should be approved or not:\nThe first is that the loan is repaid in full, and is represented by \\(loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt\\). This assumes that the loan is repaid in full after 10 years, and that the lender is able to recover 25% of the loan amount as interest after paying employees and other expenses.\nThe second is that the loan is defaulted on, and is represented by \\(loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt\\). This assumes that the loan is defaulted on after 3 years, and that the lender is able to recover 30% of the loan amount.\nWe can use these two assumptions to determine the threshold for the ‘optimal’ decision. If the predicted outcome is greater than the threshold, then the loan should be approved, otherwise it should be rejected.\n\nimport numpy as np\n\n# create functions to calculate profit and loss for an entry in the dataset\n\ndef profitORloss(row):\n    # access loan status from Y_train\n    loan_status = Y_train.loc[row.name]\n    # if loan_status is 0, calculate profit, else calculate loss\n    if loan_status == 0:\n        return row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)**10 - row['loan_amnt']\n    else:\n        return -(row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)*3 - 1.7*row['loan_amnt'])\n\n\n\n# calculate profit or loss for each entry in the dataset and add it to the new column 'profit_or_loss'\nX_train['profit_or_loss'] = X_train.apply(profitORloss, axis = 1)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n4613.567568\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n2044.334031\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n-1850.395625\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n4028.690420\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n-7765.037500\n\n\n\n\n\n\n\n\n# graphic of profit or loss for the entire dataset\nplt.hist(X_train['profit_or_loss'], bins = 20)\nplt.xlabel('Profit or Loss')\nplt.ylabel('Frequency')\nplt.title('Histogram of Profit or Loss')\nplt.show()\n\n# calculate the total profit or loss for the entire dataset\ntotal_profit_or_loss = X_train['profit_or_loss'].sum()\nprint(\"Total Profit or Loss: \", total_profit_or_loss)\n\n\n\n\n\n\n\n\n-24572766.55886856\n\n\nNow we can see the profit (or lack thereof) of each loan. Based on these assumptions, we can determine the threshold \\(t\\) for the ‘optimal’ decision, where if the profit is greater than \\(t\\), then the loan should be approved, otherwise it should be rejected. Now, lets graph the profit of each loan against different thresholds to determine the ‘optimal’ decision.\n\n# Now we can see the profit (or lack thereof) of each loan. Based on these assumptions, we can determine the threshold $t$ for the 'optimal' decision, where if the profit is greater than $t$, then the loan should be approved, otherwise it should be rejected. Now, lets graph the profit of each loan against different thresholds to determine the 'optimal' decision.\n\n# find maximum profit\nmax_profit = 0\n\nfor t in np.linspace(-5000, 5000, 100):\n    profit = X_train['profit_or_loss']\n    profit = profit[profit &gt; t]\n    profit = profit.sum()\n    plt.scatter(t, profit, color = 'blue')\n    if profit &gt; max_profit:\n        max_profit = profit\n        max_t = t\nplt.xlabel('Threshold')\nplt.ylabel('Profit')\nplt.title('Profit vs. Threshold')\n# plot the threshold at maximum profit\nplt.axvline(x = max_t, color = 'red')\nplt.show()\n\nprint(\"Threshold at maximum profit: \", max_t)\nprint(\"Maximum profit: \", max_profit)\n\n\n\n\n\n\n\n\nThreshold at maximum profit:  -1262.6262626262628\nMaximum profit:  50918882.99425644\n\n\n\n# add a new column 'decision' to the dataset\nX_train['decision'] = X_train['profit_or_loss'] &gt; max_t\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss\ndecision\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n1\n4613.567568\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n1\n0\n2044.334031\nTrue\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n1\n0\n-1850.395625\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n4028.690420\nTrue\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n1\n0\n-7765.037500\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\nWith this threshold, we can now determine the ‘optimal’ decision for each loan. If the profit is greater than the threshold, then the loan should be approved, otherwise it should be rejected. Using that decision, we can determine the overall profit of the loans and evaluate the performance of the model.\n\n# With this threshold, we can now determine the 'optimal' decision for each loan. If the profit is greater than the threshold, then the loan should be approved, otherwise it should be rejected. Using that decision, we can determine the overall profit of the loans and evaluate the performance of the model.\n\n# calculate the overall profit of the loans\noverall_profit = X_train['profit_or_loss'][X_train['decision']].sum()\nprint(\"Overall Profit: \", overall_profit)\n\n# calculate average profit per accepted loan\naverage_profit = overall_profit/X_train['decision'].sum()\nprint(\"Average Profit per Accepted Loan: \", average_profit)\n\nOverall Profit:  50918882.99425644\nAverage Profit per Accepted Loan:  2831.8159720959034\n\n\n\n\nEvaluating the Money Making Model\nNow, lets evaluate the model on the test set to see how well it performs. We will use the threshold that we determined earlier to make predictions on the test set and evaluate the performance of the model.\n\n# test model on test data\n# load test data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n# drop NAs\ndf_test = df_test.dropna()\nX_test, Y_test = df_test.drop(['loan_status'], axis = 1), df_test['loan_status']\n\n# drop loan_grade\nX_test = X_test.drop(['loan_grade'], axis = 1)\n\n# one-hot encode the categorical variables\nX_test = pd.get_dummies(X_test)\n\n# encode true/false columns as 1/0\nX_test['person_home_ownership_MORTGAGE'] = X_test['person_home_ownership_MORTGAGE'].astype(int)\nX_test['person_home_ownership_OTHER'] = X_test['person_home_ownership_OTHER'].astype(int)\nX_test['person_home_ownership_OWN'] = X_test['person_home_ownership_OWN'].astype(int)\nX_test['person_home_ownership_RENT'] = X_test['person_home_ownership_RENT'].astype(int)\nX_test['loan_intent_DEBTCONSOLIDATION'] = X_test['loan_intent_DEBTCONSOLIDATION'].astype(int)\nX_test['loan_intent_EDUCATION'] = X_test['loan_intent_EDUCATION'].astype(int)\nX_test['loan_intent_HOMEIMPROVEMENT'] = X_test['loan_intent_HOMEIMPROVEMENT'].astype(int)\nX_test['loan_intent_MEDICAL'] = X_test['loan_intent_MEDICAL'].astype(int)\nX_test['loan_intent_PERSONAL'] = X_test['loan_intent_PERSONAL'].astype(int)\nX_test['loan_intent_VENTURE'] = X_test['loan_intent_VENTURE'].astype(int)\nX_test['cb_person_default_on_file_N'] = X_test['cb_person_default_on_file_N'].astype(int)\nX_test['cb_person_default_on_file_Y'] = X_test['cb_person_default_on_file_Y'].astype(int)\n\n\n\n# predict the loan status\nY_pred = model.predict(X_test)\n\n# calculate accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of model: \", accuracy_score(Y_test, Y_pred))\n\n# ...\n\n# calculate profit or loss for each entry in the dataset and add it to the new column 'profit_or_loss'\ndef profitORloss2(row, loan_status):\n    if loan_status == 0:\n        return row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)**10 - row['loan_amnt']\n    else:\n        return -(row['loan_amnt']*(1 + .25*row['loan_int_rate']*0.01)*3 - 1.7*row['loan_amnt'])\n\n# Reset the index of X_test\nX_test.reset_index(drop=True, inplace=True)\n# Reset the index of Y_test\nY_test.reset_index(drop=True, inplace=True)\n\nX_test['profit_or_loss_predicted'] = X_test.apply(lambda row: profitORloss2(row, Y_pred[row.name]), axis=1)\n\n# add a new column 'decision' to the dataset\nX_test['decision_from_predicted'] = X_test['profit_or_loss_predicted'] &gt; max_t\n\nX_test.head()\n\nAccuracy of model:  0.800383877159309\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit_or_loss_actual\nprofit_or_loss_predicted\ndecision_from_predicted\ndecision_from_actual\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n-1416.850000\n465.367227\nTrue\nFalse\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\n1\n0\n0\n...\n0\n0\n0\n0\n1\n0\n4847.780062\n4847.780062\nTrue\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n1091.841800\n1091.841800\nTrue\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\n1\n0\n0\n...\n0\n1\n0\n0\n1\n0\n-16869.900000\n4972.214553\nTrue\nFalse\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\n0\n0\n0\n...\n0\n1\n0\n0\n1\n0\n-9933.700000\n3331.859215\nTrue\nFalse\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# calculate profit on test data\nprofit = X_test['profit_or_loss_predicted']\nprofit = profit[profit &gt; max_t]\nprofit = profit.sum()\nprofit\n\n# calculate profit on test data if all loans are approved\nprofit_all_approved = X_test['profit_or_loss_predicted'].sum()\nprint(\"Profit if all loans are approved: \", profit_all_approved)\n\n# calculate profit on test data if all loans are rejected\nprofit_all_rejected = 0\nprint(\"Profit if all loans are rejected: \", profit_all_rejected)\n\n# calculate profit on test data if we use the model to make decisions\nprofit_model = profit\nprint(\"Profit if we use the model to make decisions: \", profit_model)\n\n# improvement in profit\nimprovement = profit_model - profit_all_approved\nprint(\"Improvement in profit: \", improvement)\n\n# average profit per loan\naverage_profit = profit/len(X_test)\nprint(\"Average profit per loan: \", average_profit)\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(Y_test, X_test['decision_from_predicted'])\n\nProfit if all loans are approved:  9487460.66068708\nProfit if all loans are rejected:  0\nProfit if we use the model to make decisions:  16509810.33443708\nImprovement in profit:  7022349.67375\nAverage profit per loan:  2880.7904963247393\n\n\narray([[  77, 4377],\n       [ 210, 1067]], dtype=int64)\n\n\nThats pretty sweet! We have an accuracy of 80% on the test set, which is pretty high. We also increased the projected profits from 9487460 to 16509810, which is a 7 million dollar increase. This is a significant improvement, and shows the power of ‘optimal’ decision making in money driven computer science. It was also interesting to look at the confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives. This can help us understand where the model is making mistakes and how we can improve it, however, I’m feeling pretty good about 7 million dollars.\nAdditionally, the average profit per loan actually increased in the test set, though only marginally. It went from 2832 to 2881, so 49 dollars. I guess the money business is booming!\n\n\nEvaluating the Model Morally\nMoney is great, but now it is time to evaluate how the model might discriminate against certain groups. To do this, I will graph the distribution of rejected and accepted loans for different metrics. I will be attempting to answer 3 questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\n# Is it more difficult for people in certain age groups to access credit under your proposed system?\n# calculate the percentage of approved loans for each age\napprovedByAge = []\nfor i in range(18, 101):\n    approvedByAge.append(X_test['decision_from_predicted'][X_test['person_age'] == i].sum()/len(X_test['decision_from_predicted'][X_test['person_age'] == i]))\nactualByAge = []\n# add Y_test to X_test\nX_test['loan_status'] = abs(Y_test -1) \nfor i in range(18, 101):\n    actualByAge.append(X_test['loan_status'][X_test['person_age'] == i].sum()/len(X_test['loan_status'][X_test['person_age'] == i]))\nplt.plot(range(18, 101), approvedByAge, color = 'red')\nplt.plot(range(18, 101), actualByAge, color = 'blue')\nplt.xlabel('Age')\nplt.ylabel('Predicted vs actual repayment rate')\nplt.show()\n\n# graph of difference between predicted and actual repayment rate by age\nplt.plot(range(18, 101), np.array(approvedByAge) - np.array(actualByAge))\nplt.xlabel('Age')\nplt.ylabel('Difference between predicted and actual repayment rate')\nplt.show()\n\nC:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3012\\4166889995.py:5: RuntimeWarning: invalid value encountered in scalar divide\n  approvedByAge.append(X_test['decision_from_predicted'][X_test['person_age'] == i].sum()/len(X_test['decision_from_predicted'][X_test['person_age'] == i]))\nC:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3012\\4166889995.py:10: RuntimeWarning: invalid value encountered in scalar divide\n  actualByAge.append(X_test['loan_status'][X_test['person_age'] == i].sum()/len(X_test['loan_status'][X_test['person_age'] == i]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like younger people are expected to default more often, which is why they are rejected more often. There is also an interesting trend where older aged individuals are more likely to have loans accepted that will default. I wonder if this is because they are likely to die prior to full repayment of the loan. The implications of this are interesting, as it could be seen as a form of discrimination against both younger and older individuals, even if it is based on data. Overall though, the model does a decent job of not completely rejecting any age group, which is a good sign.\n\n# Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\n# calculate the percentage of approved loans for each loan intent\napprovedByIntent = []\nX_test['loan_intent'] = df_test['loan_intent']\nfor intent in X_test['loan_intent'].unique():\n    approvedByIntent.append(X_test['decision_from_predicted'][X_test['loan_intent'] == intent].sum()/len(X_test['decision_from_predicted'][X_test['loan_intent'] == intent]))\nactualByIntent = []\nfor intent in X_test['loan_intent'].unique():\n    actualByIntent.append(X_test['loan_status'][X_test['loan_intent'] == intent].sum()/len(X_test['loan_status'][X_test['loan_intent'] == intent]))\n\n# table of predicted vs actual repayment rate by loan intent\n\ndf = pd.DataFrame({'Loan Intent': X_test['loan_intent'].unique(), 'Predicted Repayment Rate': approvedByIntent, 'Actual Repayment Rate': actualByIntent})\nprint(df)\n\n# graph of predicted vs actual repayment rate by loan intent\n\nplt.bar(X_test['loan_intent'].unique(), approvedByIntent, color = 'red')\nplt.bar(X_test['loan_intent'].unique(), actualByIntent, color = 'blue')\nplt.xlabel('Loan Intent')\nplt.xticks(rotation = 45)\nplt.ylabel('Predicted vs actual repayment rate')\nplt.legend(['Approved', 'Actual'])\nplt.show()\n\n         Loan Intent  Predicted Repayment Rate  Actual Repayment Rate\n0            VENTURE                  0.954988               0.765207\n1          EDUCATION                  0.949564               0.804074\n2            MEDICAL                  0.947368               0.767368\n3    HOMEIMPROVEMENT                  0.947080               0.753650\n4           PERSONAL                  0.944764               0.757192\n5  DEBTCONSOLIDATION                  0.955112               0.812968\n\n\n\n\n\n\n\n\n\nIt seems like the loans for all of the different purposes are accepted at similar rates, which is a good sign. However, the actual default rates for each group are a little different. For medical expenses, model predicts that more of the loans will be repayed than they actually are. This is a good sign, but medical loans are still the third most likely to be denied but only the 4th most likely to default. However, the difference in approval for each group is not too large. Despite this, medical debt is undoubtedly a horrible burden upon those who are already suffering, and it is important to consider the additional human cost of denying these loans.\n\n# How does a person’s income level impact the ease with which they can access credit under your decision system?\n\n# divide income into 10 bins\nX_test['income_bin'] = pd.qcut(X_test['person_income'], 10, labels = False)\n\n# calculate the percentage of approved loans for each income bin\napprovedByIncome = []\nfor i in range(10):\n    approvedByIncome.append(X_test['decision_from_predicted'][X_test['income_bin'] == i].sum()/len(X_test['decision_from_predicted'][X_test['income_bin'] == i]))\nactualByIncome = []\nfor i in range(10):\n    actualByIncome.append(X_test['loan_status'][X_test['income_bin'] == i].sum()/len(X_test['loan_status'][X_test['income_bin'] == i]))\n\n# table of predicted vs actual repayment rate by income bin\n\ndf = pd.DataFrame({'Income Bin': range(10), 'Predicted Repayment Rate': approvedByIncome, 'Actual Repayment Rate': actualByIncome})\nprint(df)\n\n# graph of predicted vs actual repayment rate by income bin\n\nplt.bar(range(10), approvedByIncome, color = 'red')\nplt.bar(range(10), actualByIncome, color = 'blue')\nplt.xlabel('Income Bin')\nplt.ylabel('Predicted vs actual repayment rate')\nplt.legend(['Approved', 'Actual'])\nplt.show()\n\n   Income Bin  Predicted Repayment Rate  Actual Repayment Rate\n0           0                  0.884692               0.516899\n1           1                  0.874751               0.604374\n2           2                  0.911439               0.789668\n3           3                  0.937093               0.728850\n4           4                  0.964215               0.807157\n5           5                  0.966135               0.824701\n6           6                  0.984344               0.835616\n7           7                  0.979920               0.877510\n8           8                  0.997984               0.893145\n9           9                  1.000000               0.904573\n\n\n\n\n\n\n\n\n\nI am so proud of my model. It does a very good job of not discriminating based on income level, with lower level incomes actually having a larger disparity between accepted and defaulted loans than higher income levels. Interestingly, the model predicts that the different income levels have a somewhat similar default rate, while the actual default rate is much higher for lower income levels. This is bad from a money perspective, but good from a moral perspective, as it shows that the model is not discriminating based on income level. Additionally, while one could argue that money is being lost to the poorer individuals, this is still maximizing profit with the current threshold. So, I suppose it could be seen as profitable to be charitable, at least in this case.\n\n\nReflection\nI was surprised by the level of relative fairness achieved by my model. It was able to maximize profit while not obviously discriminating based on age or income level. It is at least subtle, which is more than I can say for most people. I learned that thresholds are important in decision making, and that they can be used to determine the ‘optimal’ decision beyond predicting labels. I also learned that it is important to consider the ethical implications of decision making, and that profit is not the only consideration when making decisions.\nIn terms of further ethical considerations, medical expenses have high default rates. This may be due to the nature that those in medical debt may not recover, and thus may not be able to repay the loan. It bring us to the question of whether it is fair to make these loans more difficult to obtain. While I feel my model was relatively successful in not discriminating against those seeking medical loans, it is still important to consider the human cost of denying a single one of these loans. Even cosmetic surgeries can be life changing for some! In the end, we must decide what is fair.\nFairness is a two part decision here, as we must decide what is means to be fair as well as apply that definition to this particular instance. I believe that fair decision making means not making decisions on things which cannot be determined by the individual. Even though we live in a “pull yourself up by your bootstraps” society, most individuals are a product of their environments, and some boots don’t even have straps anymore! Further, many things are impacted by a combination of effort and luck. However, if we can determine what it means to have something be outside of an individual’s control, we can make reasonably fair decisions.\nWe must also consider the business cost of accepting medical loans. While humans are not numbers, the individuals who make up a business are. While profits in the current setting generally go straight to the top and most certainly do not drop in the fashion of trickle down economics, in a world where they did we would have to consider the cost of accepting these loans on those who give them out. On the flip side, an individual who is able to pay off a medical loan is likely to be a loyal customer in the future.\nIn conclusion, it is not fair for medical expenses to have higher rates of default than other loans, especially when old age, which also has a high rate of default via death, is not discriminated against. The most fair decision would be to de-profitize the medical industry, but that is a discussion for another day."
  },
  {
    "objectID": "posts/logistic regression/Implementing Logistic Regression.html",
    "href": "posts/logistic regression/Implementing Logistic Regression.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "# %load_ext autoreload\n# %autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\nCSCI 0451: Implementing Logistic Regression\n\nAbstract\nThis is an implementation of Logistic Regression. In it, I explore the mathematics behind the algorithm and implement it in Python. I then test the algorithm on a dataset. I explore concepts such as momentum and overfitting using the algorithm.\nMy logistic.py code can be found at https://github.com/jnerenberg/jnerenberg.github.io/blob/main/posts/logistic%20regression/logistic.py.\n\n\nExperimental Data\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n# graph the data\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color = 'red')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color = 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHow to Train Your Dragon (Model)\n\n\nimport torch.nn as nn\n\n# Get the number of features\ninput_dim = X.shape[1]\n\n# Initialize the Logistic Regression model and the optimizer\nLR = LogisticRegression(input_dim)  # Assuming binary classification\nlearning_rate = 0.01\nopt = GradientDescentOptimizer(LR, learning_rate)\n\n# Define the loss function\nloss_function = nn.BCELoss()\n\nfor _ in range(100):\n    # Compute the loss\n    y_pred = LR(X)\n    y = y.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y)\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\n\nExperiment 1:\nVanilla gradient descent: When the number of features , when is sufficiently small and , gradient descent for logistic regression converges to a weight vector that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations).\n\n# Flatten y to a 1D tensor\ny_flat = y.flatten()\n\n# Plot the decision boundary\nx1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\nweights = LR.linear.weight.detach().numpy()  # Convert the weights to a numpy array\nx2 = -(weights[0][0] * x1 + weights[0][2]) / weights[0][1]\n\nplt.plot(x1, x2, color = 'black')\nplt.scatter(X[y_flat == 0][:, 0], X[y_flat == 0][:, 1], color = 'red')\nplt.scatter(X[y_flat == 1][:, 0], X[y_flat == 1][:, 1], color = 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# print the accuracy\ny_pred = LR(X)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y, y_pred.detach().numpy())\nprint(f'Accuracy: {accuracy}')\n\n# do the losses converge?\nlosses = []\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = LR(X)\n    y = y.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y)\n    losses.append(loss.item())\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\nplt.plot(losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration')\nplt.show()\n\n\nAccuracy: 0.92\n\n\n\n\n\n\n\n\n\nThis looks like a decent linear decision boundary. It has an accuracy of 0.94, which I am happy with. It also looks similar to the line I would have chosen if I were to draw it by hand. The loss decreases with each iteration, which is also what I would expect, even though its a little slow.\n\n\nExperiment 2:\nBenefits of momentum: On the same data, gradient descent with momentum (e.g. ) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with ). Plot the loss over iterations for each method. You may need to experiment with the data and choice of in order to observe speedups due to momentum.\n\n# Benefits of momentum: On the same data, gradient descent with momentum (e.g. ) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with ). Plot the loss over iterations for each method. You may need to experiment with the data and choice of in order to observe speedups due to momentum.\n\n# momentum\nw2 = torch.randn(input_dim, 1, requires_grad = True)\nv = torch.zeros(input_dim, 1)\ngamma = 0.9\nlearning_rate = 0.01\nlosses = []\n\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = X @ w2\n    y_pred = torch.sigmoid(y_pred)\n    loss = loss_function(y_pred, y.view(y_pred.shape))\n    losses.append(loss.item())\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    with torch.no_grad():\n        v = gamma * v + learning_rate * w2.grad\n        w2 -= v\n    \n    # Zero the gradients\n    w2.grad.zero_()\n\nplt.plot(losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration')\nplt.show()\n\n\n\n\n\n\n\n\n\n# print the accuracy\ny_pred = LR(X)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y, y_pred.detach().numpy())\nprint(f'Accuracy: {accuracy}')\n\nAccuracy: 0.9166666666666666\n\n\nWoah! This converges much faster than the vanilla gradient descent. The loss decreases much faster and the accuracy is pretty much the same. It took about a fifth of the iterations to converge to the same accuracy. This is a huge improvement.\n\n\nExperiment 3:\nOverfitting: Generate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\n\n# Overfitting: Generate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\nX_val, y_val = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.8, p_dims = 100)\n\n# Initialize the Logistic Regression model and the optimizer\ninput_dim = X_train.shape[1]\nLR = LogisticRegression(input_dim)  # Assuming binary classification\nlearning_rate = 0.01\nopt = GradientDescentOptimizer(LR, learning_rate)\n\n# Define the loss function\nloss_function = nn.BCELoss()\n\nfor _ in range(10000):\n    # Compute the loss\n    y_pred = LR(X_train)\n    y_train = y_train.view(y_pred.shape)  # Reshape y to match y_pred\n    loss = loss_function(y_pred, y_train)\n    \n    # Compute the gradients\n    loss.backward()\n    \n    # Update the model parameters\n    opt.step()\n    \n    # Zero the gradients\n    LR.zero_grad()\n\n# print the accuracy\ny_pred = LR(X_train)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_train, y_pred.detach().numpy())\nprint(f'Training accuracy: {accuracy}')\n\n# print the accuracy\ny_pred = LR(X_val)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_val, y_pred.detach().numpy())\nprint(f'Validation accuracy: {accuracy}')\n\n# print the accuracy\ny_pred = LR(X_test)\ny_pred = (y_pred &gt; 0.5).float()\naccuracy = accuracy_score(y_test, y_pred.detach().numpy())\nprint(f'Test accuracy: {accuracy}')\n\nTraining accuracy: 1.0\nValidation accuracy: 0.98\nTest accuracy: 0.88\n\n\nTraining accuracy is 1.0, which is expected since we have more features than data points. The test accuracy is 0.88, which is honestly better than I expected, but still overfitted.\n\n\n\nConclusion\nIn conclusion, logistic regression is a powerful tool for binary classification. It is daunting yet rewarding to implement from scratch. I learned a lot about the math behind it and how to optimize it. I also learned about the importance of momentum in optimization. I am happy to better understand the tools we have been using and expect it will drastically improve my usage of the tool. Also, I am very confused by my desire to say “overfat” instead of “overfitted” when using the past tense of “overfit”. Perhaps I am overfitting some other language rule to this word."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog + some extra text yay"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Project Blog Post\n\n\n\n\n\nBlog posts describing my groups cancer project \n\n\n\n\n\nMay 17, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nIn this blog post, I will replicate some of the findings from “Dissecting racial bias in an algorithm used to manage the health of populations.” \n\n\n\n\n\nMay 17, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nIn this blog post, I’ll implement a generalized form of gradient descent for logistic regression. \n\n\n\n\n\nMay 5, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nIn this blog post, I will further study optimal decision-making in the context of the the credit-risk prediction problem. My aim is to (a)determine a score function and threshold that maximize profit for the bank under more realistic assumptions and (b) assess which populations of prospective borrowers are most impacted by my proposed policy. \n\n\n\n\n\nMay 5, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nIn this blog post, I’ll work through a complete example of the standard machine learning workflow. Your primary goal is to determine the smallest number of measurements necessary to confidently determine the species of a penguin. \n\n\n\n\n\nApr 25, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nWomen in Data Science Conference\n\n\n\n\n\nThis blog post involves attending, reporting on, and reflecting on the Women in Data Science (WiDS) Conference at Middlebury College on March 4th, 2024. \n\n\n\n\n\nMar 4, 2024\n\n\nJulia Nerenberg\n\n\n\n\n\n\n\n\n\n\n\n\nReflective Goal-Setting\n\n\n\n\n\nWe plan our goals for learning, engagement, and achievement over the course of the semester! \n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html",
    "href": "posts/goal-setting-lol/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Julia Nerenberg\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am most interested in the Implementation and Theory aspects of Machine Learning. I already took a course at UMass Boston called “Ethics in Computing,” so I feel like I have the Social Responsibility section somewhat covered already.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI want to complete at least 5 blog posts. I want to make sure that I am able to write about the topics that I am most interested in, so I will focus on completing those well before stressing about completing the rest. I haven’t had many classes that have required me to critically discuss recent readings, so I am excited to have an opportunity to argue, which can be a lot of fun.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI want to show up to every class, baring any unexpected emergencies. I also want to show up with a strong attempt at the warm up activity. Sometimes, I’ll be somewhat absent from discussions if I don’t feel that I’ve adequately prepared, even if I have points to add. To combat this, I will always have at least some points prepared to bring up, so I don’t feel useless. For instance, today, I had a hard time thinking of a point to bring up, but I wrote down something I didn’t feel super confident about, and my group leader ended up mentioning it to the class. I also want to focus on this because having different perspectives is important, and I shouldn’t dismiss my ideas even if I dont initially feel they fit the mold given by the prompt.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI want to do a project on something I wouldn’t pick for myself. For instance, in my J-term class, I randomly chose the topic of my final project and ended up having a blast with it! I feel it will allow me to develop new opinions rather than just going into a topic that I already feel I know about. As a result, I would be happy with literally any topic. In terms of the group, Zoe would like to complete a cancer project, and I think she would like to do this on her own, but I have designated myself as her “back-up” group member if you won’t let her, so if you could get an answer to her on that so I can plan one way or another that would be wonderful. If I have to choose a topic, I would be intersted in an algorithm that identifies the most relevant personal information for sale for a given prompt. For instance, if I am stalking an individual and want to know their place of employment, I would want to purchase information regarding their location during work hours and maybe work associated emails. It would also be cool to have something that would pick up information in an image that can indicate where it was taken, like street signs, store signs, license plates’ state of origin, maybe even location of sun with respect to time and timezones. I am not a stalker. I just think it’d be neat."
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html#what-youll-learn",
    "href": "posts/goal-setting-lol/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am most interested in the Implementation and Theory aspects of Machine Learning. I already took a course at UMass Boston called “Ethics in Computing,” so I feel like I have the Social Responsibility section somewhat covered already."
  },
  {
    "objectID": "posts/goal-setting-lol/goal-setting.html#what-youll-achieve",
    "href": "posts/goal-setting-lol/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI want to complete at least 5 blog posts. I want to make sure that I am able to write about the topics that I am most interested in, so I will focus on completing those well before stressing about completing the rest. I haven’t had many classes that have required me to critically discuss recent readings, so I am excited to have an opportunity to argue, which can be a lot of fun.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI want to show up to every class, baring any unexpected emergencies. I also want to show up with a strong attempt at the warm up activity. Sometimes, I’ll be somewhat absent from discussions if I don’t feel that I’ve adequately prepared, even if I have points to add. To combat this, I will always have at least some points prepared to bring up, so I don’t feel useless. For instance, today, I had a hard time thinking of a point to bring up, but I wrote down something I didn’t feel super confident about, and my group leader ended up mentioning it to the class. I also want to focus on this because having different perspectives is important, and I shouldn’t dismiss my ideas even if I dont initially feel they fit the mold given by the prompt.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI want to do a project on something I wouldn’t pick for myself. For instance, in my J-term class, I randomly chose the topic of my final project and ended up having a blast with it! I feel it will allow me to develop new opinions rather than just going into a topic that I already feel I know about. As a result, I would be happy with literally any topic. In terms of the group, Zoe would like to complete a cancer project, and I think she would like to do this on her own, but I have designated myself as her “back-up” group member if you won’t let her, so if you could get an answer to her on that so I can plan one way or another that would be wonderful. If I have to choose a topic, I would be intersted in an algorithm that identifies the most relevant personal information for sale for a given prompt. For instance, if I am stalking an individual and want to know their place of employment, I would want to purchase information regarding their location during work hours and maybe work associated emails. It would also be cool to have something that would pick up information in an image that can indicate where it was taken, like street signs, store signs, license plates’ state of origin, maybe even location of sun with respect to time and timezones. I am not a stalker. I just think it’d be neat."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer penguins/Classifying-Palmer-Penguins.html",
    "href": "posts/palmer penguins/Classifying-Palmer-Penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "CSCI 0451: Classifying Palmer Penguins\n\nAbstract\nThis post is an exploration of the Palmer Penguins dataset. The dataset contains measurements of penguins from three different species. I will use the dataset to train a machine learning model to classify the species of penguins based on certain measurements. I will primarily be utilizing the pandas library, but I will also make use of the scikit-learn library to train a support vector machine (SVM) model on the dataset and the matplotlib library to visualize the data and the decision boundaries of the model.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nTime to clean the data.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nData Exploration\nHere are some exploratory visualizations of the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualization 1\n# Pairplot of the data, colored by species\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nsns.pairplot(train, hue = \"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nThis gives us a great overview of the data at a glance, but let’s take a closer look at some of the features that appear to be the most divisive between species. Seeing as the flipper length does a good job of separating the Gentoo penguins from the other two species, let’s select that at one of our initial features. We also want to look at a metric seems to isolate the two other species. Using the charts on the diagonal, we can see that delta 13 C is good for separating the Chinstraps, and the culmen length is good for separating the Adelies.\n\n# Visualization 2\n# Let's look more at the flipper length, delta 13 c, and culmen length\n# flipper length by species\nsns.boxplot(data = train, x = \"Species\", y = \"Flipper Length (mm)\")\nplt.xticks(rotation = 45)\nplt.title(\"Flipper Length by Species\")\nplt.show()\n# delta 13 c by species\nsns.boxplot(data = train, x = \"Species\", y = \"Delta 13 C (o/oo)\")\nplt.xticks(rotation = 45)\nplt.title(\"Delta 13 C by Species\")\nplt.show()\n# culmen length by species\nsns.boxplot(data = train, x = \"Species\", y = \"Culmen Length (mm)\")\nplt.xticks(rotation = 45)\nplt.title(\"Culmen Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, its time for some tables. We can use the groupby function to get the mean and median values of the flipper length, delta 13 C, and culmen length for each species. This will give us a good idea of the average values for each species, and will help us to see how the species differ in terms of these features.\n\n# Mean values\nmeans = train.groupby(\"Species\")[[\"Flipper Length (mm)\", \"Delta 13 C (o/oo)\", \"Culmen Length (mm)\"]].mean()\nprint(\"Mean values\")\nprint(means)\n\nMean values\n                                           Flipper Length (mm)  \\\nSpecies                                                          \nAdelie Penguin (Pygoscelis adeliae)                 190.084034   \nChinstrap penguin (Pygoscelis antarctica)           196.000000   \nGentoo penguin (Pygoscelis papua)                   216.752577   \n\n                                           Delta 13 C (o/oo)  \\\nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)               -25.796897   \nChinstrap penguin (Pygoscelis antarctica)         -24.553401   \nGentoo penguin (Pygoscelis papua)                 -26.149389   \n\n                                           Culmen Length (mm)  \nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                 38.970588  \nChinstrap penguin (Pygoscelis antarctica)           48.826316  \nGentoo penguin (Pygoscelis papua)                   47.073196  \n\n\n\n# Median values\nmedians = train.groupby(\"Species\")[[\"Flipper Length (mm)\", \"Delta 13 C (o/oo)\", \"Culmen Length (mm)\"]].median()\nprint(\"Median values\")\nprint(medians)\n\nMedian values\n                                           Flipper Length (mm)  \\\nSpecies                                                          \nAdelie Penguin (Pygoscelis adeliae)                      190.0   \nChinstrap penguin (Pygoscelis antarctica)                196.0   \nGentoo penguin (Pygoscelis papua)                        216.0   \n\n                                           Delta 13 C (o/oo)  \\\nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                -25.89709   \nChinstrap penguin (Pygoscelis antarctica)          -24.59467   \nGentoo penguin (Pygoscelis papua)                  -26.20455   \n\n                                           Culmen Length (mm)  \nSpecies                                                        \nAdelie Penguin (Pygoscelis adeliae)                      38.9  \nChinstrap penguin (Pygoscelis antarctica)                49.3  \nGentoo penguin (Pygoscelis papua)                        46.5  \n\n\nWhile these metrics are a great start, it is important to remember that human decision making is not always ideal or feasible. We can instead use some machine learning tools to help us determine the best features to use.\nLet’s perform an exhaustive search over all possible feature combinations to determine the best features to use for our model. We will use the combinations function from the itertools package for this.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Define the model\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 0)\n\n# Define the features\nfeatures = X_train.columns\n\n# Define the number of features to select\nn_features = 3\n\n# Perform the search\nbest_score = 0\nbest_features = []\nfor combo in combinations(features, n_features):\n  X_train_subset = X_train[list(combo)]\n  score = cross_val_score(model, X_train_subset, y_train, cv = 5).mean()\n  if score &gt; best_score:\n    best_score = score\n    best_features = combo\n\nprint(\"Best features: \", best_features)\nprint(\"Best score: \", best_score)\n\nBest features:  ('Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_MALE')\nBest score:  0.9844645550527904\n\n\nWow! That took a long time (over a minute!). It’s important to note that this method will not work for larger datasets, as the number of possible combinations grows exponentially with the number of features. However, for this dataset, it is a feasible method.\nTaking a look at the results, we can see that the best features to use are Culmen Length, Culmen Depth, and Sex. Time to train the model.\n\n\nModel Time\nTo start, lets define our Logistic Regression model. We will use the LogisticRegression class from the scikit-learn library to do this.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Define the model\nmodel = LogisticRegression(max_iter = 1000)\n\n# Define the features\nfeatures = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Sex_MALE\", \"Sex_FEMALE\"]\n\n# Train the model\nmodel.fit(X_train[features], y_train)\nprint(\"Train Set Score: \", model.score(X_train[features], y_train))\n\n# Let's now evaluate the model on the test set.\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nprint(\"Test Set Score: \", model.score(X_test[features], y_test))\n\n\nTrain Set Score:  0.9921875\nTest Set Score:  0.9852941176470589\n\n\nSuccess! Our model has an accuracy of 98% on the test data. This is a great result, but it is important to remember that this is a small dataset, and the model may not perform as well on larger datasets. However, for this dataset, the model is performing very well.\nNext up, let’s visualize the decision boundaries of the model.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Ensure axarr is a list\n    if not isinstance(axarr, np.ndarray):\n        axarr = np.array([axarr])\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(model, X_train[features], y_train)\n\n\n\n\n\n\n\n\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_pred = model.predict(X_test[features])\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, fmt = \"d\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\n\n\n\n\n\n\nThe above confusion matrix indicates that we have pretty great results. We are mostly getting the correct species, with only a few misclassifications. We can see from the graph that the misclassifications are mostly due to special penguin specimens who have outlying features. This is good because it means that our model is likely not overfitting to the training data. Lets verify this using cross-validation.\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X_train, y_train, cv = 5)\nscores.mean()\nprint(\"Cross Validation Score: \", scores.mean())\n\nCross Validation Score:  1.0\n\n\n\n\nDiscussion\nIn this post, I explored the differences between the three species of penguins in the Palmer Penguins dataset. I used the dataset to train a machine learning model to classify the species of penguins based on certain measurements. Some of the measurements were quantitative while others were qualitative. I was able to use the combinations function from the itertools package to determine the best features to use for the model. I then trained a logistic regression model from the scikit-learn library on the dataset and visualized the decision boundaries of the model. The model was highly accurate and performed well on the test data. I also used cross-validation to verify that the model was not overfitting to the training data. Overall, the model performed very well on the dataset, and the results were promising."
  },
  {
    "objectID": "posts/Replication Study/Replication-Study.html",
    "href": "posts/Replication Study/Replication-Study.html",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "CSCI 0451: Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\nAbstract\nThis post is an exploration of the findings of the paper, “Dissecting racial bias in an algorithm used to manage the health of populations.” It explores the implications of racial bias in healthcare algorithms and the potential consequences of using these algorithms in practice. The paper highlights the importance of understanding the biases present in these algorithms and the need for more research to address these issues. I will replicate some of the findings of the paper and provide a critical analysis of the results.\n\n\nPart A: Data Access\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\n# Exploring the data\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n# look at important columns: risk_score_t, cost_t, race, and gagne_sum_t\ndf2 = df[['risk_score_t', 'cost_t', 'race', 'gagne_sum_t']].copy()\ndf2.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\n# graphing the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# print number of black and white patients\nprint(df2['race'].value_counts())\n\n# plot the data\nsns.pairplot(df2)\n\n\nrace\nwhite    43202\nblack     5582\nName: count, dtype: int64\n\n\nc:\\Users\\julia\\anaconda3\\envs\\ML-2000\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nc:\\Users\\julia\\anaconda3\\envs\\ML-2000\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nc:\\Users\\julia\\anaconda3\\envs\\ML-2000\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\nPart B: Reproduce Figure 1\nThis figure visualizes risk score percentiles against mean number of active chronic conditions for black and white patients.\n\n# graph risk score percentiles against mean number of active chronic conditions for black and white patients\n\n# get the percentiles\ndf2['risk_score_percentile'] = round(df['risk_score_t'].rank(pct=True), 2) * 100\n\n# get the mean number of active chronic conditions and reset the index\ndf3 = df2.groupby(['risk_score_percentile','race'])['gagne_sum_t'].mean()\n\ndf3 = pd.DataFrame(df3)\n\n# plot the data\nsns.scatterplot(data=df3, y='risk_score_percentile', x='gagne_sum_t', hue = 'race')\nplt.xlabel('Mean number of chronic conditions')\nplt.ylabel('Risk score percentile')\nplt.title('Mean number of chronic conditions vs. risk score percentile for black and white patients')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot demonstrates the racial bias present in the risk score algorithm. The risk scores for black patients are consistently higher than those for white patients, even when the number of active chronic conditions is the same. This indicates that the algorithm is biased against black patients, assigning them higher risk scores than white patients with the same health conditions.\n\n\nPart C: Reproduce Figure 3\nThis figure visualizes how total medical expenditures are correlated with the risk score and with the number of chronic health conditions using two panels. The first panel plots total medical expenditure against percentile risk score and the second panel plots total medical expenditure against the number of chronic conditions.\n\n# This figure visualizes how total medical expenditures are correlated with the risk score and with the number of chronic health conditions using two panels. The first panel plots total medical expenditure against percentile risk score and the second panel plots total medical expenditure against the number of chronic conditions.\n\n# get the mean total medical expenditures\ndf4 = pd.DataFrame(df2.groupby(['risk_score_percentile','race'])['cost_t'].mean())\ndf5 = pd.DataFrame(df2.groupby(['gagne_sum_t','race'])['cost_t'].mean())\n\n# plot the data\nfig, axs = plt.subplots(1, 2, sharey=True)\n\nsns.scatterplot(data=df4, x='risk_score_percentile', y='cost_t', hue = 'race', ax=axs[0])\nsns.scatterplot(data=df5, x='gagne_sum_t', y='cost_t', hue='race', ax=axs[1])\n\n# set labels\naxs[0].set_xlabel('Risk score percentile')\naxs[0].set_ylabel('Total medical expenditures')\naxs[1].set_xlabel('Number of chronic conditions')\n\n# legend stuff\naxs[0].legend(loc='upper left')\naxs[1].legend().remove()\n\nplt.yscale('log')\nplt.show()\n\n\n\n\n\n\n\n\nThese plots demonstrate that total medical expenditures increase with higher risk scores and a greater number of chronic conditions. However, black patients with the same risk score had lower medical expenditures than white patients, indicating that the algorithm may be underestimating the healthcare needs of black patients. Interestingly, it appears black patients make up a large portion of the outliers in the first panel, indicating that in instances where they pay more than white patients, they tend to pay much more. This could be also indicate that the risk algorithm is poorly tuned for black patients.\n\n\nPart D: Modeling Cost Disparity\n\n# determine the percentage of patients in the data with 5 or fewer chronic conditions\ndf6 = df2[df2['gagne_sum_t'] &lt;= 5]\npercentage = len(df6)/len(df2)*100\nprint(\"Percentage of patients with 5 or fewer chronic conditions: \", percentage)\n\nPercentage of patients with 5 or fewer chronic conditions:  95.53952115447689\n\n\nThis percentage does indicate that we should focus on patients with 5 or fewer chronic conditions, as they make up the majority of the population. However, we need to be careful not to ignore patients with more chronic conditions in later stages of the analysis, as they may have higher medical expenditures and require more attention.\n\n# Create a new column of the data set which is just the logarithm of the cost. This is called a log-transform. We’ll use this as our target variable. Log transforms are common when the target variable varies widely across several orders of magnitude. Because log(0) is undefined, you should subset the data so that patients who incurred $0 in medical costs are removed.\n\ndf2 = df2[df2['cost_t'] &gt; 0]\ndf2.loc[:, 'log_cost'] = df2['cost_t'].apply(lambda x: np.log(x))\n\n# replace inf values with NaN\ndf2.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# graph log cost against risk score percentile\ndf7 = pd.DataFrame(df2.groupby(['risk_score_percentile'])['log_cost'].mean())\nsns.scatterplot(data=df7, x='risk_score_percentile', y='log_cost')\nplt.xlabel('Risk score percentile')\nplt.ylabel('Log cost')\nplt.title('Log cost vs. risk score percentile')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a dummy (one-hot encoded) column for the qualitative race variable in which 0 means that the patient is white and 1 means that the patient is Black\ndf2.loc[:,'race_dummy'] = df2['race'].apply(lambda x: 1 if x == 'black' else 0)\n\ndf2.head(15)\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_score_percentile\nlog_cost\nrace_dummy\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n0\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n0\n\n\n5\n4.450484\n123700.0\nwhite\n1\n68.0\n11.725615\n0\n\n\n6\n4.195685\n12900.0\nwhite\n1\n66.0\n9.464983\n0\n\n\n7\n0.713436\n3900.0\nwhite\n0\n10.0\n8.268732\n0\n\n\n8\n1.087141\n1000.0\nblack\n1\n18.0\n6.907755\n1\n\n\n9\n0.832342\n200.0\nwhite\n0\n12.0\n5.298317\n0\n\n\n10\n0.713436\n1500.0\nwhite\n0\n10.0\n7.313220\n0\n\n\n11\n3.940887\n21500.0\nwhite\n1\n63.0\n9.975808\n0\n\n\n12\n0.866316\n800.0\nwhite\n0\n13.0\n6.684612\n0\n\n\n13\n1.919484\n2800.0\nwhite\n0\n34.0\n7.937375\n0\n\n\n14\n0.407678\n2100.0\nwhite\n0\n4.0\n7.649693\n0\n\n\n\n\n\n\n\n\n# Separate the data into predictor variables X and target variable y (the log-cost). For predictor variables, just use the dummy columns for race and the number of active chronic conditions.\n\nX = df2[['race_dummy', 'gagne_sum_t', 'risk_score_percentile', 'risk_score_t']]\ny = df2['log_cost']\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X, y)\n\n# add polynomial features\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n# loop through polynomial features of varying degrees using add_polynomial_features and cross_val_score to determine the best degree\nmax_score = 0\nfor i in range(1, 25):\n  X_ = add_polynomial_features(X, i)\n  scores = cross_val_score(LR.fit(X_, y), X_, y, cv=5).mean()\n  print(f\"Degree {i}: {scores.mean()}\") \n  if scores &gt; max_score:\n    max_score = scores\n    best_degree = i\n\nprint(f\"Best degree: {best_degree} at {max_score}\")\n\n\n\n\n\nDegree 1: 0.25539613788482207\nDegree 2: 0.25539613788482207\nDegree 3: 0.25564888929939117\nDegree 4: 0.25609938363207574\nDegree 5: 0.25607069827694195\nDegree 6: 0.25625955333137707\nDegree 7: 0.2564252446267551\nDegree 8: 0.25640003372393394\nDegree 9: 0.2564242169488689\nDegree 10: 0.2564533053746847\nDegree 11: 0.25639384376814056\nDegree 12: 0.2563983605338217\nDegree 13: 0.25635481290198764\nDegree 14: 0.25626679553993736\nDegree 15: 0.25465260347421387\nDegree 16: 0.24480386175973914\nDegree 17: 0.12346457985251931\nDegree 18: 0.10911644750197495\nDegree 19: 0.10944262978112854\nDegree 20: 0.11049373279577526\nDegree 21: 0.11000738094929408\nDegree 22: 0.10855748113346249\nDegree 23: 0.11429892205035311\nDegree 24: 0.10851654074884584\nBest degree: 10 at 0.2564533053746847\n\n\nThe polynomial with degree 10 had the maximum cross validation score at .2564.\n\n# construct a copy of the data with this correct number of polynomial features and fit one last linear regression model\n\nX_ = add_polynomial_features(X, best_degree)\nLR.fit(X_, y)\n\n# get the coefficients\ncoefficients = LR.coef_\nprint(coefficients)\n\n[-1.94142978e-01  2.22302356e-01  1.38644478e-02  3.72898504e-02\n  2.22302370e-01 -6.38916760e-01  3.80208341e-01 -1.14270884e-01\n  1.98729469e-02 -2.07784555e-03  1.28373619e-04 -4.30575642e-06\n  6.03040560e-08]\n\n\nThe weight corresponding with race is the first weight in the list, which is -1.94142978e-01.\n\n# calculate e^(w_b)\n\ne_wb = np.exp(coefficients[0])\nprint(e_wb)\n\n\n0.8235401479795127\n\n\nBased on the results of the model, black patients pay, on average, about 82.3% of what white patients pay on healthcare. Black patients paying less than their white counterparts supports the paper’s argument that black patients receive less healthcare on average, despite having similar numbers of chronic conditions.\n\n\nConclusion\nThis blog post has allowed me to replicate some of the findings of a vastly important and relevant paper in the field of healthcare algorithms. The results of the analysis demonstrate the presence of racial bias in the risk score algorithm and its impact on healthcare expenditures. The findings highlight the need for further research to address these biases and ensure that healthcare algorithms are fair and equitable for all patients, regardless of race. The paper provides valuable insights into the implications of racial bias in healthcare algorithms and the potential consequences for patient care, stressing the need for unbiased algorithms that provide equitable care suggestions for all patients.\nThe criteria best describing the purported bias of the algorithm studied by Obermeyer et al. (2019) are risk score accuracy and fairness. The algorithm assigns higher risk scores to black patients than white patients with the same health conditions, indicating that it is not accurate in predicting generalized healthcare needs. This inaccuracy leads to disparities in healthcare expenditures between black and white patients, which is unfair and discriminatory. The study provides evidence of these disparities through visualizations and statistical analysis, demonstrating the impact of racial bias on patient care. The findings of the study highlight the importance of addressing these biases and developing algorithms that are accurate and fair for all patients, regardless of race."
  }
]